{"video": {"id": "_C8kWso4ne4", "title": "PySpark Tutorial", "description": "Learn PySpark, an interface for Apache Spark in Python. PySpark is often used for large-scale data processing and machine learning.\n\n\ud83d\udcbb Code: https://github.com/krishnaik06/Pyspark-With-Python\n\n\u270f\ufe0f Course from Krish Naik. Check out his channel: https://youtube.com/user/krishnaik06\n\n\u2328\ufe0f (0:00:10) Pyspark Introduction\n\u2328\ufe0f (0:15:25) Pyspark Dataframe Part 1\n\u2328\ufe0f (0:31:35) Pyspark Handling Missing Values\n\u2328\ufe0f (0:45:19) Pyspark Dataframe Part 2\n\u2328\ufe0f (0:52:44) Pyspark Groupby And Aggregate Functions\n\u2328\ufe0f (1:02:58) Pyspark Mlib And Installation And Implementation\n\u2328\ufe0f (1:12:46) Introduction To Databricks\n\u2328\ufe0f (1:24:65) Implementing Linear Regression using Databricks in Single Clusters\n\n--\n\n\ud83c\udf89 Thanks to our Champion and Sponsor supporters:\n\ud83d\udc7e Wong Voon jinq\n\ud83d\udc7e hexploitation\n\ud83d\udc7e Katia Moran\n\ud83d\udc7e BlckPhantom\n\ud83d\udc7e Nick Raker\n\ud83d\udc7e Otis Morgan\n\ud83d\udc7e DeezMaster\n\ud83d\udc7e Treehouse\n\n--\n\nLearn to code for free and get a developer job: https://www.freecodecamp.org\n\nRead hundreds of articles on programming: https://freecodecamp.org/news", "duration": "PT1H49M2S", "likes": "11322", "views": "634256"}, "comments": [{"topLevelComment": {"author": "Pallab M", "text": "Hi,  thanks for this tutorial, If my dataset has 20 columns, why describe output is not showing in a nice table like the above? It is coming all distorted. Is there a way to get a nice tabular format like above for a large dataset?", "likes": 0}}, {"topLevelComment": {"author": "Mimi Zheng", "text": "Anyone getting a \"Runtime Error: Java gateway process exited before seding its port number\"  error?  How do I rectify this?", "likes": 0}, "replies": [{"author": "Mimi Zheng", "text": "Got it to work! You need to create a JAVA_HOME variable and set it to where java sits in your computer.  Of course, make sure you have java installed in your system", "likes": 0}]}, {"topLevelComment": {"author": "Siddhant Bhagat", "text": "I am very happy to see krish sir on this channel.", "likes": 0}}, {"topLevelComment": {"author": "Aditi Verma", "text": "Is it necessary to learn python before learning pyspark?", "likes": 0}}, {"topLevelComment": {"author": "Syed Asgar Ahmed", "text": "I am getting an error...\nRuntimeError: Java gateway process exited before sending its port number", "likes": 0}}, {"topLevelComment": {"author": "nitesh agrahari", "text": "Hi Krish I am getting this error\nRuntimeError: Java gateway process exited before sending its port number\nafter running spark=SparkSession.builder.appName('Practise').getOrCreate()", "likes": 0}}, {"topLevelComment": {"author": "cheese cake", "text": "ERROR on start session:\n\nRuntimeError: Java gateway process exited before sending its port number", "likes": 0}}, {"topLevelComment": {"author": "YARRAMNEEDI RAVINDRASWAMY", "text": "it showing below error.\ncould you please help me in this\n\n\n\n\n      running install_lib\r\n      copying build\\lib\\pyspark\\python\\pyspark\\shell.py -> C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\pyspark\r\n      byte-compiling C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\examples\\src\\main\\python\\ml\\multiclass_logistic_regression_with_elastic_net.py to multiclass_logistic_regression_with_elastic_net.cpython-310.pyc\r\n      error: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\pyspark\\\\examples\\\\src\\\\main\\\\python\\\\ml\\\\__pycache__\\\\multiclass_logistic_regression_with_elastic_net.cpython-310.pyc.2644507691136'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> pyspark\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\n\r\nC:\\Users\\HP>cd C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\examples\\src\\main\\python\\ml\\__pycache__\\multiclass_logistic_regression_with_elastic_net.cpython-310.pyc.2644507691136", "likes": 0}}, {"topLevelComment": {"author": "YARRAMNEEDI RAVINDRASWAMY", "text": "hello sir, when I try to install pyspark it shows below thing is missing in my command prompt\n\\multiclass_logistic_regression_with_elastic_net.cpython-310.pyc.2644507691136", "likes": 0}}, {"topLevelComment": {"author": "Jos\u00e9 Vitor", "text": "Thank you so much for an amazing tutorial session!\ud83d\ude80\ud83d\ude80\ud83d\ude80", "likes": 0}}, {"topLevelComment": {"author": "sonam kori", "text": "Thank You", "likes": 0}}, {"topLevelComment": {"author": "Jas", "text": "This is a great view on coding. Can you add some interview questions?", "likes": 0}}, {"topLevelComment": {"author": "Patrick El Zaybak", "text": "how to fix this error:\npy4j.protocol.Py4JJavaError: An error occurred while calling o45.load.", "likes": 0}}, {"topLevelComment": {"author": "Zohan Syah Fatomi", "text": "Thank you for the tutorial. I have one question. Is hadoop has similar role with pyspark? Please let me know.", "likes": 0}}, {"topLevelComment": {"author": "stkmgr00", "text": "I am using ipynb \nat line \"spark=SparkSession.builder.appName('test').getOrCreate()\" \ngetting following error. \nPy4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)", "likes": 0}}, {"topLevelComment": {"author": "Nirmalya Roy", "text": "while creating spark session I am getting \"RuntimeError: Java gateway process exited before sending its port number\". could you please help me", "likes": 0}}, {"topLevelComment": {"author": "Himanshu_ Kumar", "text": "What should I do?\n\n\n\n\n\nRuntimeError                              Traceback (most recent call last)\r\nInput In [3], in <cell line: 1>()\r\n----> 1 spaark=SparkSession.builder.appName('Pactise').getOrCreate()\r\n\r\nFile ~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:269, in SparkSession.Builder.getOrCreate(self)\r\n    267     sparkConf.set(key, value)\r\n    268 # This SparkContext may be an existing one.\r\n--> 269 sc = SparkContext.getOrCreate(sparkConf)\r\n    270 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\r\n    271 # by all sessions.\r\n    272 session = SparkSession(sc, options=self._options)\r\n\r\nFile ~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:483, in SparkContext.getOrCreate(cls, conf)\r\n    481 with SparkContext._lock:\r\n    482     if SparkContext._active_spark_context is None:\r\n--> 483         SparkContext(conf=conf or SparkConf())\r\n    484     assert SparkContext._active_spark_context is not None\r\n    485     return SparkContext._active_spark_context\r\n\r\nFile ~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:195, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\r\n    189 if gateway is not None and gateway.gateway_parameters.auth_token is None:\r\n    190     raise ValueError(\r\n    191         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\r\n    192         \" is not allowed as it is a security risk.\"\r\n    193     )\r\n--> 195 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\r\n    196 try:\r\n    197     self._do_init(\r\n    198         master,\r\n    199         appName,\r\n   (...)\r\n    208         udf_profiler_cls,\r\n    209     )\r\n\r\nFile ~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:417, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\r\n    415 with SparkContext._lock:\r\n    416     if not SparkContext._gateway:\r\n--> 417         SparkContext._gateway = gateway or launch_gateway(conf)\r\n    418         SparkContext._jvm = SparkContext._gateway.jvm\r\n    420     if instance:\r\n\r\nFile ~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py:106, in launch_gateway(conf, popen_kwargs)\r\n    103     time.sleep(0.1)\r\n    105 if not os.path.isfile(conn_info_file):\r\n--> 106     raise RuntimeError(\"Java gateway process exited before sending its port number\")\r\n    108 with open(conn_info_file, \"rb\") as info:\r\n    109     gateway_port = read_int(info)\r\n\r\nRuntimeError: Java gateway process exited before sending its port number", "likes": 0}}, {"topLevelComment": {"author": "Artemis", "text": "Thanks", "likes": 0}}, {"topLevelComment": {"author": "Dendi Handian", "text": "when we read csv and do some agregation in the spark dataframe, does it really use spark engine? or it just performing pandas process in the inside?\n\nhow to we monitor or see that we actually using spark engine when doing something in the notebook?", "likes": 0}}, {"topLevelComment": {"author": "Sandeep Nelwade", "text": "I got error when create sparksession\nlike Namerror : name 'spark' is not defined", "likes": 0}}, {"topLevelComment": {"author": "Doreyed Ahmed", "text": "Thank you so much\n\nvery nice explanation\n\nIf you use  pyspark, its consider we deal with  Spark Apache", "likes": 0}}, {"topLevelComment": {"author": "Yogita Hande", "text": "I have pyspark code now need to convert it into CaaS spark 3.x series, can you please guide me on this", "likes": 0}}, {"topLevelComment": {"author": "DevSkills", "text": "Mention, as a prerequisite for this session apache-spark must be installed in your system.", "likes": 0}}, {"topLevelComment": {"author": "\u0418\u0432\u0430\u043d \u0421\u0435\u0434\u043e\u0432", "text": "\u041f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u043e\u0435 \u0432\u0438\u0434\u0435\u043e \u0438 \u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u0430\u044f \u043c\u0430\u043d\u0435\u0440\u0430 \u043f\u043e\u0434\u0430\u0447\u0438 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u0430. \u0411\u043e\u043b\u044c\u0448\u043e\u0435 \u0441\u043f\u0430\u0441\u0438\u0431\u043e!", "likes": 4}}, {"topLevelComment": {"author": "gayan ranasinghe", "text": "Truth is this pyspark tutorial could have been lot better,this definitely needs a lot of improvement and doesn't meet the standards to be in free code camp channel.why I say so,49:00 minutes past , still doing the basic pyspark stuff", "likes": 0}}, {"topLevelComment": {"author": "Saurabh Katiyar", "text": "I think (df.na.fill()) , function has been changed. Integer column does not accept the str values.", "likes": 0}}, {"topLevelComment": {"author": "Zizou", "text": "Dear Krish. This is only W.O.N.D.E.R.F.U.L.L \ud83d\ude09.\nThanks so Much and thanks to professor  Hayth....  who showed me the link to your training. Cheers to both of U guys", "likes": 1}}, {"topLevelComment": {"author": "Meghana Sreeram", "text": "How to can coverage report for pyspark unit test cases sir", "likes": 0}}, {"topLevelComment": {"author": "Gina", "text": "I followed all of the directions but spark never returns outputs", "likes": 0}}, {"topLevelComment": {"author": "Ashish Dhere", "text": "RuntimeError: Java gateway process exited before sending its port number\n\ni am facing the error", "likes": 0}}, {"topLevelComment": {"author": "ANTON MURSID", "text": "Antonmursid\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\u270c\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\u270c\ud83d\udc9d\ud83d\udc4c\ud83d\ude4f", "likes": 1}}, {"topLevelComment": {"author": "ANTON MURSID", "text": "Antonmursid\n\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\u270c\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\u270c\ud83d\udc9d\ud83d\udc4c\u270c\ud83d\ude4f", "likes": 1}}, {"topLevelComment": {"author": "ANTON MURSID", "text": "Antonmursid\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\u270c\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\u270c\ud83d\udc9d\ud83d\udc4c\ud83d\ude4f", "likes": 1}}, {"topLevelComment": {"author": "RJ Hari", "text": "AttributeError: 'DataFrame' object has no attribute 'withMetadata' help me with it #krish", "likes": 0}}, {"topLevelComment": {"author": "Adithya Vishwa", "text": "Where is the .csv file? Do you need to upload it first to your jupyter notebook first before opening it?", "likes": 0}}, {"topLevelComment": {"author": "IA", "text": "Hi Sir, I am facing issue, how we can write dataframe values to kafka using df.write.format? and my column value of dataFrame has data like [{\"a\":\"1\"}, {\"b\":\"2\"} ] so i want to push this data as it is to kafka", "likes": 0}}, {"topLevelComment": {"author": "Rohan C", "text": "If anyone is having trouble with getting the SparkSession instantiation to run at 9:38 and is using Visual Studio Code on Windows 10 like me, make sure to have Spark installed first, and then run VSCode as administrator.", "likes": 0}}, {"topLevelComment": {"author": "SkateForLife", "text": "Nice video, clear and precise. But it would be better with better dataset, to show more options in the data analysis (grouping more columns, max(column) etc.)", "likes": 3}}, {"topLevelComment": {"author": "Muhammad Ashir Ali", "text": "How to groupby  sum a specific column?", "likes": 0}}, {"topLevelComment": {"author": "Yite Zeng", "text": "I have to say, it is nice and clear. The pace is really good as well. There are many tutorials online that are either too fast or too slow.", "likes": 1}}, {"topLevelComment": {"author": "janhvi vora", "text": "spark =SparkSession.builder.appName('Practise').getOrCreate()\nRuntimeError: Java gateway process exited before sending its port number  \nThankyou in advance, how to solve this error?", "likes": 0}}, {"topLevelComment": {"author": "Suman Mukherjee", "text": "Hello,\nimputer function and replace null values are not happening for me , still showing me null only , what to do ??\n### Filling the Missing _Value\ndf_pyspark.na.fill('Missing Values',['Experience','age']).show()", "likes": 0}}, {"topLevelComment": {"author": "Harshvardhan Pareek", "text": "On my notebook, it is only replacing null values as 'missing values' on Name column, on others it is still showing null. What could be the issue ?", "likes": 0}}, {"topLevelComment": {"author": "Julio Serratos", "text": "Beatiful.", "likes": 0}}, {"topLevelComment": {"author": "Prudhvi Vardhan", "text": "Helpful \ud83d\udc4f", "likes": 0}}, {"topLevelComment": {"author": "kailash kangne", "text": "solution for Py4JJavaError : https://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}}, {"topLevelComment": {"author": "Aymen Lamzouri", "text": "Very nice video, one question is how do you get this help window that displays the input of the functions that you are using ?", "likes": 0}}, {"topLevelComment": {"author": "APOORVA KULKARNI KULKARNI", "text": "Hey for this line \nspark=SparkSession.builder.appName('Practise').getOrCreate() \ngetting \nRuntimeError: Java gateway process exited before sending its port number \nerror", "likes": 0}, "replies": [{"author": "kailash kangne", "text": "read 3 instruction in below video\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "Lakshmi Reddy", "text": "how to solve py4j error .anyone  plz tell me", "likes": 0}, "replies": [{"author": "kailash kangne", "text": "https://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "K-land  healing", "text": "\uc751\uc6d0\ud569\ub2c8\ub2e4~^^\n-\uc0ac\ub791\ud574\uc694-\n\uc54c\ub78c\ud544\ud588\uc5b4\uc694~^^", "likes": 0}}, {"topLevelComment": {"author": "Ujjawal Handa", "text": "There is an update in na.fill(), any integer value inside fill will replace nulls from columns having integer data types and so for the case of string value as well.", "likes": 3}, "replies": [{"author": "Austin Chettiar", "text": "@Harsha Leo so whats the exact keyword to replace all null values?", "likes": 0}, {"author": "Harsha Leo", "text": "Yeah. If we are trying to fill with a string, it is filling only the Name column nulls.", "likes": 0}]}, {"topLevelComment": {"author": "Aditya Rastogi", "text": "It is taking forever to start spark session\ncan anyone help on this?", "likes": 0}, "replies": [{"author": "kailash kangne", "text": "read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "Faris Hambo", "text": "Hvala!", "likes": 0}}, {"topLevelComment": {"author": "Tyro Elohim", "text": "Hello krish, getting error here. \n\ndf_pyspark.groupBy('Name').sum().show()", "likes": 0}}, {"topLevelComment": {"author": "jibba jABBA", "text": "can we get 1 tutorial vid without some corny dude with a thick hindi accent...", "likes": 1}, "replies": [{"author": "kailash kangne", "text": "no", "likes": 0}]}, {"topLevelComment": {"author": "Rohan Oxob3000", "text": "40:00", "likes": 0}}, {"topLevelComment": {"author": "Daniel", "text": "TOP, thank you so much!", "likes": 0}}, {"topLevelComment": {"author": "Venkat Kondragunta", "text": "Hey Krish, Thank you so much for your efforts.. this is really helpful..", "likes": 0}}, {"topLevelComment": {"author": "Pavan", "text": "Before anything for people getting error , you need to do this os.environ['JAVA_HOME']= 'C:\\Program Files\\Java\\jre1.8.0_333' inside your jupyter notebook or inside conda termnial , check your Java path first in system and give", "likes": 0}, "replies": [{"author": "Pavan", "text": "sometime it will happen that your system itself dont have this path set , inside windows you need go to system variables and set JAVA_HOME = C:\\Program Files\\Java\\jre1.8.0_333 and save", "likes": 0}]}, {"topLevelComment": {"author": "AMN", "text": "48:00 Filters", "likes": 0}}, {"topLevelComment": {"author": "Ask Doubts", "text": "na.file(col name)  - is applicable only on name filed only for me, if i tried on any other filed it's not working.\n\ndf_pysparknull.na.fill('Miss','name').show()\r\n\n+---------+----+----------+------+\r\n|     Name| age|Experience|Salary|\r\n+---------+----+----------+------+\r\n|    Krish|  31|        10| 30000|\r\n|Sudhanshu|  30|         8| 25000|\r\n|    Sunny|  29|         4| 20000|\r\n|     Paul|  24|         3| 20000|\r\n|   Harsha|  21|         1| 15000|\r\n|  Shubham|  23|         2| 18000|\r\n|   Mahesh|null|      null| 40000|\r\n|     Miss|  34|        10| 38000|\r\n|     Miss|  36|      null|  null|\r\n+---------+----+----------+------+\n\ndf_pysparknull.na.fill('Miss','age').show()\n\n+---------+----+----------+------+\r\n|     Name| age|Experience|Salary|\r\n+---------+----+----------+------+\r\n|    Krish|  31|        10| 30000|\r\n|Sudhanshu|  30|         8| 25000|\r\n|    Sunny|  29|         4| 20000|\r\n|     Paul|  24|         3| 20000|\r\n|   Harsha|  21|         1| 15000|\r\n|  Shubham|  23|         2| 18000|\r\n|   Mahesh|null|      null| 40000|\r\n|     null|  34|        10| 38000|\r\n|     null|  36|      null|  null|\r\n+---------+----+----------+------+", "likes": 0}}, {"topLevelComment": {"author": "Haonan Liu", "text": "In the final example, are you trying to predict the total bill according to all these other factors, instead of predicting the tips?", "likes": 0}}, {"topLevelComment": {"author": "Raunak ghosh", "text": "Hi, i tried running the type(df_pyspark) but its providing an output of \"nonetype\" instead of \"dataframe\". Can you please suggest what i should be doing?", "likes": 2}, "replies": [{"author": "Raunak ghosh", "text": "@ranroun3 Thank you. Real helpful tip", "likes": 0}, {"author": "ranroun3", "text": "just fixed it. remove the '.show()' from the df_pyspark declaration (line 13 in his version). The .show command apparently makes the object a NoneType", "likes": 2}, {"author": "ranroun3", "text": "having the same issue", "likes": 0}]}, {"topLevelComment": {"author": "Miguel Perez", "text": "I just love how he says\n\n\u201cVery very simple guys\u201d\n\nAnd it turns out to be simple xD", "likes": 1}}, {"topLevelComment": {"author": "Ranjeeth Rikkala", "text": "Need your data files to watch this video", "likes": 0}}, {"topLevelComment": {"author": "ame rayan", "text": "when a spark session is created the following error occurs\n\n\r\nAttributeError                            Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_2904/2592031109.py in <module>\r\n----> 1 spark = SparkSession.builder.appname('practice').getOrCreate()\r\n\r\nAttributeError: 'Builder' object has no attribute 'appname'\r\n\nhow to debug this error sir", "likes": 0}}, {"topLevelComment": {"author": "Dotan Gabay", "text": "does anyone got this? appName() missing 1 required positional argument: 'name'. on 10:05", "likes": 0}}, {"topLevelComment": {"author": "bhanuprakash chavva", "text": "HI, How to create a new environment", "likes": 0}}, {"topLevelComment": {"author": "MSN", "text": "I'm getting erroe for \nspark = SparkSession.Builder.appName('Practice').getOrCreate()\nas\nTypeError: appName() missing 1 required positional argument: 'name'", "likes": 0}}, {"topLevelComment": {"author": "Sameel Jabir", "text": "Such an amazing explanation.\nFor a beginner: 1.50!hours really worth...\n\nYou nailed it in a way with very simple examples In high professional way....\n\nHuge Hatsoff", "likes": 4}}, {"topLevelComment": {"author": "programming_duck", "text": "Min 58:23 to show the maximum salary you should use max instead of sum? sum will work because name is unique, but i found this a bit misleading.", "likes": 0}}, {"topLevelComment": {"author": "Yikun Sun", "text": "Dude is so funny!!!! Great video anyway!", "likes": 0}}, {"topLevelComment": {"author": "Subham Kumar", "text": "Hi how can i read in pyspark zipped json files would be thankful for any help @Krish Naik", "likes": 0}}, {"topLevelComment": {"author": "Anoop Srivastava", "text": "Hi Krisk Naik, I am facing issue while running \"spark = SparkSession.builder.appName('Practice').getOrCreate()\" as its taking a lot of time and the kernel gets hanged there itself ?", "likes": 0}}, {"topLevelComment": {"author": "christsciple", "text": "I receive the following error: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ when trying to run spark = sparkSession.builder.appName('Practise').getOrCreate()\nResearching on Google suggests its an issue with the version of Java JDK I'm running. I've tried 18, 11, and now 8 and run into the same issue. Anyone know the solution?", "likes": 1}, "replies": [{"author": "kailash kangne", "text": "read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "Krishna Chaitanya Reddy", "text": "is there second part", "likes": 0}}, {"topLevelComment": {"author": "GAURANG AGARWAL", "text": "I am getting error while starting sparksession , can sombody help me out?", "likes": 0}, "replies": [{"author": "kailash kangne", "text": "read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "Mayur Gupta", "text": "how to change the datatype in pyspark", "likes": 0}}, {"topLevelComment": {"author": "Ali Yusifov", "text": "Thank you so much for an amazing tutorial session! Easy to follow", "likes": 0}}, {"topLevelComment": {"author": "Manuel Gil", "text": "This is gold!", "likes": 0}}, {"topLevelComment": {"author": "Red Rum", "text": "so pyspark is basically like normal python for crazy large datasets,, cool!", "likes": 0}}, {"topLevelComment": {"author": "Vignesh Jaisankar", "text": "RuntimeError: Java gateway process exited before sending its port number - Do I need to install java in my laptop to avoid this error. Kindly help me", "likes": 1}, "replies": [{"author": "Sanjay Balikar", "text": "@Vignesh Jaisankar Thank you", "likes": 0}, {"author": "Vignesh Jaisankar", "text": "@Sanjay Balikar yeah ... Java was not there.. installing java in my PC resolved the issue", "likes": 2}, {"author": "Sanjay Balikar", "text": "did you get answer for this", "likes": 0}]}, {"topLevelComment": {"author": "Ad A", "text": "I'm getting error after executing this command at timestamp 9:51\nspark=SparkSession.builder.appName('Practise').getOrCreate()\nerror - \nPy4JError: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM\nplease help to solve this error", "likes": 0}, "replies": [{"author": "kailash kangne", "text": "read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}]}, {"topLevelComment": {"author": "Leandro", "text": "bro, theres a hard 'cut' in 1:12:34", "likes": 0}}, {"topLevelComment": {"author": "kwame saidi", "text": "Hello Sir \nThanks for the nice tutorial however am facing below error when trying to import pyspark\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-2-49d7c4e178f8> in <module>\n----> 1 import pyspark\n\n~\\anaconda3\\lib\\site-packages\\pyspark\\__init__.py in <module>\n     51 \n     52 from pyspark.conf import SparkConf\n---> 53 from pyspark.rdd import RDD, RDDBarrier\n     54 from pyspark.files import SparkFiles\n     55 from pyspark.status import StatusTracker, SparkJobInfo, SparkStageInfo\n\n~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py in <module>\n     41 from pyspark.rddsampler import RDDSampler, RDDRangeSampler, RDDStratifiedSampler\n     42 from pyspark.storagelevel import StorageLevel\n---> 43 from pyspark.resource.requests import ExecutorResourceRequests, TaskResourceRequests\n     44 from pyspark.resource.profile import ResourceProfile\n     45 from pyspark.resultiterable import ResultIterable\n\nModuleNotFoundError: No module named 'pyspark.resource'", "likes": 0}}, {"topLevelComment": {"author": "Chenkun Tan", "text": "Nice tutorial!", "likes": 0}}, {"topLevelComment": {"author": "Pushti Kapadia", "text": "ParseException: \r\nmismatched input '>=' expecting {<EOF>, '-'}(line 1, pos 14)\r\n\r\n== SQL ==\r\nNo. of Orders >=10\r\n--------------^^^\r\n\ncan anyone please help me to solve this error?", "likes": 0}}, {"topLevelComment": {"author": "Lydia Jones", "text": "That's Great! Thank you so much.", "likes": 0}}, {"topLevelComment": {"author": "FoXs-G", "text": "1:33:44 while data coloum sex = female", "likes": 0}}, {"topLevelComment": {"author": "Pushti Kapadia", "text": "AnalysisException: cannot resolve 'No.` of Orders`' given input columns: [Date, No. of Orders, Port, Product, Service];\r\n'Aggregate [unresolvedalias(avg(cast(CASE WHEN ('No. of Orders = NaN) THEN null WHEN isnan('No. of Orders) THEN null ELSE 'No. of Orders END as double)), Some(org.apache.spark.sql.Column$$Lambda$3268/1923570036@117f27f7))]\r\n+- Relation [Date#1061,Port#1062,No. of Orders#1063,Service#1064,Product#1065] csv\r\n\nI am facing this error while replacing the missing values with mean ! \nCan you please help me out to solve this error ?", "likes": 0}}, {"topLevelComment": {"author": "Pradip Awasthi", "text": "I want a serious help from you..actually I have a sequence data i.e., a single row data I want to split the data into multiple rows after every 5th delimiter ('|') how can I do that??", "likes": 0}}, {"topLevelComment": {"author": "Welder mm", "text": "0:52:44 - complementing  Pyspark Groupby And Aggregate Functions\r\n\ndf3 = df3.groupBy(\r\n  \"departaments\"\r\n).agg(\r\n  sum(\"salary\").alias(\"sum_salary\"),\r\n  max(\"salary\").alias(\"max_salary\"),\r\n  min('salary').alias(\"min_salary\")\r\n)", "likes": 0}}, {"topLevelComment": {"author": "ash wath", "text": "great tutorial !!", "likes": 0}}, {"topLevelComment": {"author": "Srinivas r", "text": "Thank you for such a wonderful course:)\nspark = SparkSession.Builder.appName(\"Practice\").getOrCreate()\nypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-94862321d8a1> in <module>\r\n----> 1 spark = SparkSession.Builder.appName(\"Practice\").getOrCreate()\r\n\r\nTypeError: appName() missing 1 required positional argument: 'name'\r\n I am unable to find error can you pls help", "likes": 0}}, {"topLevelComment": {"author": "Abdullah -", "text": "Hi Please can someone give a roadmap for being a junior data engineer i have learnt python and currently learning sql after i finish i will cover data warehousing is the any other area i should focus.\n\n\nThanks,", "likes": 0}}, {"topLevelComment": {"author": "Ajit Jadhav", "text": "Hi, I'm only able to replace the null values of the 'Name' column, in other columns it doesn't get changed when I use: df_pyspark.na.fill('Missing Values', ['Name', 'Age', 'Experience']).show()\n\nCan anyone help me with this?", "likes": 0}, "replies": [{"author": "Pradeep Varanasi", "text": "I believe it has something to do with the data type. For instance, if you put 0 in the above code, all the integer/numeric missing values are replaced with 0. It could be an update from the previous version.", "likes": 0}]}, {"topLevelComment": {"author": "Santosh BT", "text": "At 1:22:32, it was not running just because it was detached to any cluster?", "likes": 0}}, {"topLevelComment": {"author": "Saadat Mahmud", "text": "36:11  getting rid of null values just gonna leave this here for later on", "likes": 0}}, {"topLevelComment": {"author": "jakir adf", "text": "Super", "likes": 0}}, {"topLevelComment": {"author": "Tanya Duggal", "text": "Getting the error \"RuntimeError: Java gateway process exited before sending its port number\" while running the command \r\nspark = SparkSession.builder.appName('Practice').getOrCreate()", "likes": 1}, "replies": [{"author": "kailash kangne", "text": "@Ashwin Singh read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}, {"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}, {"author": "Ashwin Singh", "text": "I am also getting py4J error while creating session.", "likes": 0}, {"author": "subash voleti", "text": "hi did your issue got resolved. i am currently facing the similar issue now. if resolved pls ping here", "likes": 0}]}, {"topLevelComment": {"author": "Kevin Alexis", "text": "Starts 6:30", "likes": 0}}, {"topLevelComment": {"author": "D\u01b0\u01a1ng Tr\u1ea7n", "text": "thank you from Vietnam", "likes": 0}}, {"topLevelComment": {"author": "Harish Puligundla", "text": "Hey Krish, the appName is not changing for me while I create sparksession. It is always coming as 'pyspark-shell'. What should I do to fix this issue?", "likes": 0}}, {"topLevelComment": {"author": "Santhosh Kumar matlapudi", "text": "Sir, please explain why we are creating spark in the starting", "likes": 0}}, {"topLevelComment": {"author": "Anass rtimi", "text": "Thank you for this course", "likes": 0}}, {"topLevelComment": {"author": "TheRedGauntlet", "text": "Thank You for this. But im having some weird problem where i import a csv file but everything is inside one column. I tried making on excel a data set and even downloading a made one and still kept importing it like it was on one column.", "likes": 0}}, {"topLevelComment": {"author": "Manasi Aminbhavi", "text": "I have a scenario, where I want to convert input multiline json file with multiple json objects to comma separated json objects json.. Could you help how we can do this?", "likes": 0}}, {"topLevelComment": {"author": "Durgadevi Arulrajan", "text": "Hi, I Thanks for your great videos. Would like to know, why to use pyspark, when we can do all those functionalities with pandas? what is the advantages of pyspark over pandas. what are the circumstances both should be used??", "likes": 1}, "replies": [{"author": "ThePresistence", "text": "@Idris Chakera Wonderful explanation", "likes": 1}, {"author": "Idris Chakera", "text": "There are certain limitations with pandas, it can be safely used for 1-5gb data as \r\npandas loads all the data in memory.\r\nBeyond that, lets say for 5-30gbs of ,  you can still work with pandas but you have to specify a chunk_size so that it loads a small chunk of that data in your memory. \r\nFor 30-200gbs of data , you can use DASK framework that supports numpy pandas and sklearn to run them on parallel processing.\r\nBut as soon as you talk about terabytes or petabytes of data, you need to use spark for handling the data.", "likes": 5}]}, {"topLevelComment": {"author": "Deep Ghodasara", "text": "I having on question:- Code which are not with respect to spark session like codes of pandas and all how it will run under spark as it's not part of pyspark and will not run on distributed manner. \n\nCan any one explain this.", "likes": 0}}, {"topLevelComment": {"author": "Moon Cake", "text": "installation steps unclear. Unable to get it set up in my local windows machine", "likes": 0}}, {"topLevelComment": {"author": "Muhammad Sabahuddin", "text": "After running spark=SparkSession.builder.appName(\u2018Practise\u2019).getOrCreate()\nI gor error Java gateway exited before sending its port number\nAnyone knows how i fix this error?", "likes": 1}, "replies": [{"author": "kailash kangne", "text": "@Vitor Azambuja read 3 instruction in below video\r\nhttps://www.youtube.com/watch?v=nSIzZeuC9pY&ab_channel=kailashkangne", "likes": 0}, {"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}]}, {"topLevelComment": {"author": "Janardhana Polapragada", "text": "excellent\ncan i get ur nbr pl", "likes": 0}}, {"topLevelComment": {"author": "Indian Programmer", "text": "Anyone else faced issue while working with imputer function at 44:32 ??", "likes": 0}}, {"topLevelComment": {"author": "Yuper Soutuber", "text": "pls talk about error and exception handling and logging.", "likes": 0}}, {"topLevelComment": {"author": "NQ,", "text": "When will u make tutorial for Cython?\nIts fast version of python and for me its the great start while u are davanced in python so u can understand other programmung langauge.\nSorry for my english", "likes": 0}}, {"topLevelComment": {"author": "Sudhanshu Singh", "text": "I see my name in the groupby example data :D", "likes": 0}}, {"topLevelComment": {"author": "Simileoluwa Aluko", "text": "Great man. Great! \ud83d\udc4d\ud83c\udffc\ud83d\udc4d\ud83c\udffc\ud83d\udc4d\ud83c\udffc\ud83d\udc4d\ud83c\udffc", "likes": 0}}, {"topLevelComment": {"author": "Mustapha Jalo", "text": "I am getting the below error when I run the code the code import imputer, does anyone know how I can fix this \n\n\nImportError: cannot import name 'imputer' from 'pyspark.ml.feature' (C:\\Users\\A159979\\Miniconda3\\lib\\site-packages\\pyspark\\ml\\feature.py)", "likes": 0}}, {"topLevelComment": {"author": "Raj vashisth sharma", "text": "Hi Krish Naik,  I am  getting the error while using pyspark==>  \"Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\r\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.storage.StorageUtils\". PLease help me on this, i have tried all steps", "likes": 0}}, {"topLevelComment": {"author": "Amit Kumar Saha", "text": "Amazing content", "likes": 0}}, {"topLevelComment": {"author": "Siva's_Scope", "text": "if i use your code, i could see all my source data display in single column even my source file has multiple columns\nkindly help us", "likes": 0}}, {"topLevelComment": {"author": "Carlos Roberto Morales Sanchez", "text": "You dropped this king \ud83d\udc51", "likes": 1}}, {"topLevelComment": {"author": "nikunj thakkar", "text": "I'm getting an error\n\"Java gateway process exited before sending it's port number\"\nPlease suggest to overcome this error. \n\nThanks", "likes": 0}}, {"topLevelComment": {"author": "Muskan Moolchandani", "text": "Sir, i am facing the issue..when i created this spark object ..it is throwing error like\n'java gateway process exited before sending its port number'\n\nI tried multiple time to solve this problem but i couldn't able to.\nSo please help me with this issue", "likes": 0}, "replies": [{"author": "Ankita Sahoo", "text": "Install Java 8", "likes": 0}]}, {"topLevelComment": {"author": "Hemant Garg", "text": "Hey I am new to python , can anyone tell me why we used ( !pip install ) here instead of just (pip install),I  know it might be silly but maybe i will get to know something from this\ud83d\ude05\ud83d\ude05", "likes": 0}}, {"topLevelComment": {"author": "Sound Collective", "text": "This is pretty much a very useful video ;)\nthanks", "likes": 0}}, {"topLevelComment": {"author": "Celebritas", "text": "Hello guys, some one can help me how to configure jupyter in spark? Which  variables environnement can I add in order  to fix jupyter? Thanks a lot for your reply", "likes": 0}}, {"topLevelComment": {"author": "Chengzhai Wang", "text": "Quick question: when I tried code \" df_pyspark.na.fill('Missing Values')\", it only replaced the null values in the first column. Any comments? Thanks!", "likes": 0}, "replies": [{"author": "Gabriel Hernandez", "text": "same error, it is because it is only filling the string column (name) with string missing values. It wont replace integer column nulls(age , experience) with a string or you will get mixed datatypes", "likes": 0}]}, {"topLevelComment": {"author": "Hobbesian Thinker", "text": "Thanks for your tutorial. I am taking a course on PySpark to complete my micro-masters in data science. Much appreciated\ud83d\udc4d", "likes": 0}}, {"topLevelComment": {"author": "ShowBhik", "text": "25:15 You can use df_pyspark[['Name]] like in Pandas and it would work. For Selection of multiple columns, you can do: df_pyspark[['Name', 'Age']]. This is also similar to the Pandas' way of doing it.", "likes": 4}, "replies": [{"author": "somisetty revanth", "text": "it works. Thanks", "likes": 0}]}, {"topLevelComment": {"author": "varun kumar", "text": "Can anyone help me how to convert excel XLS to excel XlSX is data bricks using pyspark", "likes": 0}}, {"topLevelComment": {"author": "Kiran Todekar", "text": "RuntimeError: Java gateway process exited before sending its port number\r\n\nI am getting this error after executing the getCreate command", "likes": 1}, "replies": [{"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}]}, {"topLevelComment": {"author": "Sathishkumar Ramadas", "text": "Dear Beau, Could you please advise on the below error message as I am a beginner for python and pyspark.", "likes": 0}}, {"topLevelComment": {"author": "Bhavin Moriya", "text": "What is cicd pipeline? Thanks for the awesome vdeo:)", "likes": 0}}, {"topLevelComment": {"author": "Akash k", "text": "Thank you so much sir, 100 % satisfied with your tutorial. Loved it.", "likes": 2}}, {"topLevelComment": {"author": "Joe Stallano", "text": "Pretty much simple.", "likes": 0}}, {"topLevelComment": {"author": "Shrouk Elwakel", "text": "how do you show description of a function?", "likes": 0}}, {"topLevelComment": {"author": "Gaurav Kungwani", "text": "amazing tutor !!", "likes": 0}}, {"topLevelComment": {"author": "suriya b", "text": "I  getting Java exception handling error when I tried to execute d code. \nI installed java also.. \n I tried to solve on net also, but I could not\n. Kindly help me to solve this issue", "likes": 1}, "replies": [{"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}]}, {"topLevelComment": {"author": "Anup Gupta", "text": "Always getting error #This Spark context may be an existing one.", "likes": 0}}, {"topLevelComment": {"author": "Mahesh Kumar", "text": "Can anybody suggest me good resources like this for big data?", "likes": 0}}, {"topLevelComment": {"author": "Deeksha Bajpai", "text": "I m getting error while creating app it is showing file does not exist .Why is it so ???", "likes": 0}}, {"topLevelComment": {"author": "F_B", "text": "Two hours to show something you can read in 10 minutes..", "likes": 0}}, {"topLevelComment": {"author": "Crazy Nikhil", "text": "Indians are the best teachers in the world. Thank you :)", "likes": 0}}, {"topLevelComment": {"author": "Vin Rayudu", "text": "Mike Dane would have been good, Krish isn't really explaining the logic & reason, he is just reading the code, this course is good for just copy paste code!", "likes": 0}}, {"topLevelComment": {"author": "lokesh nagireddy", "text": "Sir, How to enable auto suggessions in Jupyter notebook..", "likes": 0}}, {"topLevelComment": {"author": "Elias Sarkis", "text": "Hello Guys,\n\nI installed pyspark through pip3 and also installed java SE 17.0.1\nbut still, have this error coming executing a code:\n\nPy4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\r\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.storage.StorageUtils$\n\nany idea please ?", "likes": 0}, "replies": [{"author": "He Dongying", "text": "i have the same error, could you please share answer if you manage to resolve the issue?", "likes": 0}]}, {"topLevelComment": {"author": "FT T", "text": "This is very basic.", "likes": 0}}, {"topLevelComment": {"author": "innovationsCode", "text": "Massive. This is a GREAT piece. Well done. Keep going", "likes": 0}}, {"topLevelComment": {"author": "Rajiv Bandaru", "text": "I just got allocated to a new project in a new organisation that I had recently joined. They mostly work on  pyspark and aws. So they asked me to learn the same. I referred their portal and found difficult to learn, but I came across this and trust me this is the easiest tutorials ever to come regarding pyspark. Kudos to you guys.", "likes": 1}, "replies": [{"author": "Kartech industries", "text": "Where in mindtree?", "likes": 0}, {"author": "krimino ahmt", "text": "did you find some interesting course on aws?", "likes": 0}]}, {"topLevelComment": {"author": "tradeKing", "text": "At 26:37 , Min and Max values from a column of string data type were not based on the index where they were placed, but it is based on their ASCII values of the words ,their order of characters that are arranged within and the order is\n' 0 < 9 <  \"A\" < \"Z\" < \"a\" < \"z\" '.\n Min will be letter comes first and Max will be which comes last of all the characters, if two similar characters found, it moves to next character and checks and so on ...", "likes": 1}, "replies": [{"author": "\ud83e\ude93 Patrick Bateman", "text": "True", "likes": 0}]}, {"topLevelComment": {"author": "Kshitij Sahdev", "text": "1:22:08 you swallowed a booger, didn't you", "likes": 0}}, {"topLevelComment": {"author": "Nzar Sharif", "text": "At 1:09:00 when you try to add Independent feature I get the below error:\n\n\n\nPy4JJavaError                             Traceback (most recent call last)\r\n<ipython-input-9-4dd4d21f583c> in <module>\r\n      1 output = featureassembler.transform(trainning)\r\n----> 2 output.show()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py in show(self, n, truncate, vertical)\r\n    492 \r\n    493         if isinstance(truncate, bool) and truncate:\r\n--> 494             print(self._jdf.showString(n, 20, vertical))\r\n    495         else:\r\n    496             try:", "likes": 1}}, {"topLevelComment": {"author": "Neil Lunavat", "text": "OMG, LETS GOOOOOOOOO", "likes": 0}}, {"topLevelComment": {"author": "\u0410\u0437\u0430\u043c\u0445\u043e\u043d \u0411\u0443\u0437\u0443\u0440\u0443\u043a\u043e\u0432", "text": "good indian accent", "likes": 0}}, {"topLevelComment": {"author": "Nirnay Roy", "text": "pretty much amazing!!!", "likes": 0}}, {"topLevelComment": {"author": "anuchuri sravanthi", "text": "Facing issue at creating spark session........", "likes": 0}}, {"topLevelComment": {"author": "Nora Abdul", "text": "In 12:50 what did you click to show that window ?", "likes": 0}}, {"topLevelComment": {"author": "msmith33897", "text": "When should you consider using PySpark over Pandas? Is there a general data size(maybe GBs) where it would be beneficial to use one over the other?", "likes": 4}, "replies": [{"author": "\ud83e\ude93 Patrick Bateman", "text": "There is a video on youtube with title \"Pandas Limitations - Pandas vs Dask vs PySpark - DataMites Courses \"  \nI can not provide the URL link since Youtube can't allow it.", "likes": 3}]}, {"topLevelComment": {"author": "Jorge", "text": "The full installation of PySpark was omitted in this course.", "likes": 5}}, {"topLevelComment": {"author": "Shriti Shaw", "text": "Sir Krish Naik is an amazing tutor, learned a lot about statistics and data science from his channel", "likes": 40}}, {"topLevelComment": {"author": "Kanwal Zahoor", "text": "I am having this error Exception: Java gateway process exited before sending its port number", "likes": 2}, "replies": [{"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}]}, {"topLevelComment": {"author": "Diederik Knaapen", "text": "I was very much looking for this. Great work, thank you!", "likes": 1}}, {"topLevelComment": {"author": "akshay agarwal", "text": "I was having the error  \"Exception: Java gateway process exited before sending its port number\". This was due to having JAVA 17.\nI uninstalled JAVA 17 and installed JAVA 8 and it worked.", "likes": 4}, "replies": [{"author": "Paras Pandhare", "text": "Hey, Can you send me the syntax to isnatll JAVA 8, coz I am too facing the issue", "likes": 0}]}, {"topLevelComment": {"author": "Miracle", "text": "how to process large scale data with spark? How to process 1TB dataset on spark?", "likes": 0}}, {"topLevelComment": {"author": "brownmunde10", "text": "One can do slicing in PySpark not exactly the way it is done in Pandas. \n\nEg.\nSyntax : df_pys.collect()[2:6]\n\nOutput : \n[Row(Name='C', Age=42),\r\n Row(Name='A2', Age=43),\r\n Row(Name='B2', Age=15),\r\n Row(Name='C2', Age=78)]", "likes": 14}, "replies": [{"author": "RAJAT BHATHEJA", "text": "However one thing is that take precaution while using collect. collect is an action and will execute your DAG.", "likes": 0}, {"author": "programming_duck", "text": "Thank you really useful", "likes": 0}]}, {"topLevelComment": {"author": "Ph\u01b0\u1edbc B\u1ea3n \u0110\u00e0o", "text": "Thank you so much.", "likes": 0}}, {"topLevelComment": {"author": "Srikrishna kumaran", "text": "what is slicing?", "likes": 0}}, {"topLevelComment": {"author": "KUNSOTH VENKATESH", "text": "Hi where I need to keep csv Or excel file?", "likes": 0}}, {"topLevelComment": {"author": "Vicky Mishra", "text": "i need help in aws glue job can you plz help me", "likes": 0}}, {"topLevelComment": {"author": "Vivek Adithya Mohankumar", "text": "I ran into an issue while importing pyspark(Import Error) in my notebook even after installing it within the environment. After doing some research, I found that the kernel used by the notebook, would be the default kernel, even if the notebook resides within virtual env. We need to create a new kernel within the virtual env, and select that kernel in the notebook.\n\nSteps:\n1. Activate the env  by executing \"source bin/activate\" inside the environment directory\n2. From within the environment, execute \"pip install ipykernel\" to install IPyKernel\n3. Create a new kernel by executing \"ipython kernel install --user --name=projectname\"\n4. Launch jupyter notebook\n5. In the notebook, go to Kernel > Change kernel and pick the new kernel you created. \n\nHope this helps! :)", "likes": 16}}, {"topLevelComment": {"author": "Belen Ortiz", "text": "donde puedo ver los Notebooks?", "likes": 0}}, {"topLevelComment": {"author": "Akshay Deshpande", "text": "Thank you  that was very helpful", "likes": 0}}, {"topLevelComment": {"author": "minh phan", "text": "ERROR: Java gateway process exited before sending its port number\n\nI got this issue when running Spark Session, but can't find any solutions for MacOs. I'm using M1 chip, please help me. Thanks in advance", "likes": 0}, "replies": [{"author": "minh phan", "text": "@Kazekage Tech Not yet bro :( still stucking", "likes": 0}, {"author": "Kazekage Tech", "text": "did yousolve", "likes": 0}]}, {"topLevelComment": {"author": "Ruth Fehilly", "text": "love this tutorial!", "likes": 0}}, {"topLevelComment": {"author": "Barzhikev Il", "text": "For the filling exercise on minute 42:00 aprox, I cannot do it with integer type data, I had to use string data like you did. But them in the next exercise, the one on minute 44:00, the function won't run unless you use integer data for the columns you are trying to fill.", "likes": 3}, "replies": [{"author": "\u6570\u8bf4\u771f\u76f8 Data Driven Australian Life and Money", "text": "@Cafe Racer Kid \ud83d\ude80 you can try to read with/without inferSchema = True and check the schema, you will see the difference. Try to read again for Imputer.", "likes": 0}, {"author": "Cafe Racer Kid \ud83d\ude80", "text": "how did you reversed to string data? I'm also facing the same issue", "likes": 0}]}, {"topLevelComment": {"author": "Eshwaran Venkat", "text": "Thanks!", "likes": 1}, "replies": [{"author": "Ujir Ali", "text": "You 5ioooppeweeetyiiop0", "likes": 0}]}, {"topLevelComment": {"author": "ming lu", "text": "in the linear regression part shouldn't be all the categorical cols transform into dummy variables?  yes for binary categorical variables it doesn't matter. But which method should be used for multi-categorial variables? stringindexer only transfer them into int numbers, which doesn't make any sense for the coef-estimation... is there another StringIndexer like method?", "likes": 0}}, {"topLevelComment": {"author": "Shubham Gupta", "text": "I am trying to initialize my spark setup but I am facing this error. I did look at stackoverflow but the issue has been same. Can anyone help\nPython: Current version 3.8\nPyspark :3.0.3\n\nPy4JError: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM", "likes": 1}}, {"topLevelComment": {"author": "Bruce Choi", "text": "I love this pyspark course!", "likes": 0}}, {"topLevelComment": {"author": "Aditya kumar", "text": "Thumbnail says full course but it is not a full course for sure.", "likes": 0}}, {"topLevelComment": {"author": "micha\u0142 botor", "text": "drop if p% (or more) are null:\n```\np = 0.7\nncols = len(dfs.columns)\nthresh = round(p * ncols)\nprint('thresh:', thresh)\ndfs.na.drop(thresh=thresh).show()\n```", "likes": 0}}, {"topLevelComment": {"author": "DEVENDRA TANDON", "text": "What to do when inferschema and structschema  both ain't working?", "likes": 0}}, {"topLevelComment": {"author": "Arulmouzhi Ezhilarasan", "text": "Impeccable Teaching! Thanks!", "likes": 1}}, {"topLevelComment": {"author": "Uboom123", "text": "Hey Krish, thanks for simple training on pyspark, can you add sample video merging data frame? And add rows to data frame?", "likes": 7}}, {"topLevelComment": {"author": "Jitender Kumar", "text": "Can u please show an example where we will be getting data from sql server database instead of any csv files. Or how to get data from sql server db to perfrom some operation related to ML. Video stream timing 1.23 where we are getting data from csv file.", "likes": 0}, "replies": [{"author": "jeremiah ishaya", "text": "getting data from an SQL server is not much different from loading from a CSV file... All you need do is send a query to the database by passing the query strings and then converting  to a data frame...\nconnection = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=.\\SQL2K19;DATABASE=SampleDB;Trusted_Connection=yes;')", "likes": 0}]}, {"topLevelComment": {"author": "Long De", "text": "Exception: Java gateway process exited before sending its port number issue and cant find resolution", "likes": 0}}, {"topLevelComment": {"author": "Ace", "text": "Great video!\nAt the beginnig you missed:\nfrom pyspark.context import SparkContext", "likes": 0}}, {"topLevelComment": {"author": "Aditya Mathur", "text": "The syntax and  the operations are such because, the data is stored in-memory in columnar fashion, where all the column headers are the Keys and all the data is the Value for the dictionary?", "likes": 0}}, {"topLevelComment": {"author": "MrAhmedUA", "text": "you can also do this \nread.csv('user.csv', header=True)", "likes": 1}}, {"topLevelComment": {"author": "linglong0419", "text": "Would like to ask why the datatype of age, experience and salary in tutorial 3 are inferred as string? I turn inferSchema to true, and these fields are inferred as int.", "likes": 0}}, {"topLevelComment": {"author": "alonso kuan", "text": "Thank you (y)", "likes": 0}}, {"topLevelComment": {"author": "Matheus Silva", "text": "it was all i needed. Thanks a lot!", "likes": 0}}, {"topLevelComment": {"author": "Lorenzo Rondan", "text": "Nice video, could you add timestamps?", "likes": 2}}, {"topLevelComment": {"author": "Lucas Schroeder Rossi", "text": "Great content! Thanks! Regards from Brazil!!!", "likes": 1}}, {"topLevelComment": {"author": "Sushil Kamble", "text": "10:00 | Whoever is getting Exception: Java gateway process exited before sending the driver its port error, Install Java SE 8 (Oracle). The error will be solved.", "likes": 1}, "replies": [{"author": "Vitor Azambuja", "text": "Me too. Did you manage to solve this problem?", "likes": 0}, {"author": "Kazekage Tech", "text": "did you solve bro? im facing it now", "likes": 0}]}, {"topLevelComment": {"author": "Taco Bell", "text": "bad English, hard to listen", "likes": 0}}, {"topLevelComment": {"author": "Miguel Jr", "text": "Someone with the same error \"Exception: Java gateway process exited before sending its port number\n\" after spark = SparkSession.builder.appName('Practise').getOrCreate() ?????", "likes": 4}, "replies": [{"author": "Miguel Jr", "text": "@Manohar Nath Gupta awesome!", "likes": 0}, {"author": "Manohar Nath Gupta", "text": "@Miguel Jr thanks mate, issue is resolved once I download java latest versions", "likes": 1}, {"author": "Miguel Jr", "text": "@Manohar Nath Gupta of course man... I watched this video but is in Portuguese-Brasil (but it is possible to follow the instructions) https://youtu.be/7tDOUrl7Aoc \n\nThat video helped me a lot to use it, good lucky buddy", "likes": 1}, {"author": "Manohar Nath Gupta", "text": "Getting same error, pls can anyone help me to resolve this issue", "likes": 0}, {"author": "Miguel Jr", "text": "@Pedro I needed to see other video explain how to install Java... Apache spark ... Configurate... And install pyspark... That was a tough thing to do ....", "likes": 1}]}, {"topLevelComment": {"author": "brahim ghaouthi", "text": "I wish you covered spark SQL as well", "likes": 0}}, {"topLevelComment": {"author": "Ankush A Gowda", "text": "In tutorial 3 u are mention that infershcema= true but age and expirience is takeing as string", "likes": 0}}, {"topLevelComment": {"author": "Golden water", "text": "Good content but you have to work on reducing the video length without sacrificing  quality.", "likes": 0}}, {"topLevelComment": {"author": "krishna kumar", "text": "FeeCodeCamp has very good reputation and don't loose it by allowing this fake people teaching, he is fraud and copy paste guy", "likes": 1}}, {"topLevelComment": {"author": "chandra mamillapalli", "text": "Can you please upload Full coarse on pyspark in one video..", "likes": 0}}, {"topLevelComment": {"author": "sanjay bohr", "text": "Getting this error \"Java gateway process exited before sending its port number\" after \"spark=SparkSession.builder.appName('Practise').getOrCreate()\"", "likes": 0}}, {"topLevelComment": {"author": "Hindustan Graphics", "text": "Hi sir . I have a doubt in python.\r\nMy_list =[\"apple\",\"mango\",\"brown sugar\"]\r\nValues =[5,  10 ,  6]\r\nI want match each value to the respected string in 'my_list' variable . It is possible using for loops . Please tell me the method.", "likes": 0}, "replies": [{"author": "Hindustan Graphics", "text": "@Mohd Abdul Azeem very very thankyou.it worked me.", "likes": 0}, {"author": "Mohd Abdul Azeem", "text": "zip(My_list, Values)", "likes": 0}]}, {"topLevelComment": {"author": "Raghunandan Reddy", "text": "Exception: Java gateway process exited before sending its port number", "likes": 3}, "replies": [{"author": "Kazekage Tech", "text": "@Miguel Jr didyou solve", "likes": 0}, {"author": "Miguel Jr", "text": "@Raghunandan Reddy Did you install JAVA? I think it s the problem... because I've never used Java before and I'm having the same error.", "likes": 0}, {"author": "Raghunandan Reddy", "text": "@VIVEK N  Not Yet Solved If you know please help me", "likes": 0}, {"author": "VIVEK N", "text": "how did u solve the error?", "likes": 1}]}, {"topLevelComment": {"author": "Karthik Avula", "text": "Thanks for this awesome session. #LoveSpark", "likes": 0}, "replies": [{"author": "Yashsvi Dixit", "text": "@Karthik Avula real tech lovers", "likes": 0}, {"author": "Karthik Avula", "text": "@Yashsvi Dixit who doesnt", "likes": 0}, {"author": "Yashsvi Dixit", "text": "don't lie dude, you don't love Spark, you just love money", "likes": 1}]}, {"topLevelComment": {"author": "satyajitsingh pardeshi", "text": "Can any one help me to solve below issue from databricks spark\r\nI have date column in below format and in string type from source ,i want convert it in to date type and normal date format.Given type conversion syntax is not supporting this date format or i am missing something dont know.Can anyone please suggest how to convert it to normal any date format.\r\n\r\nDate Jul 22, 2021 Jul 20, 2021 Jul 19, 2021 Jul 16, 2021 Jul 15, 2021 Jul 14, 2021 Jul 13, 2021 Jul 12, 2021 Jul 09, 2021 Jul 08, 2021 Jul 07, 2021 Jul 06, 2021 Jul 05, 2021 Jul 02, 2021 Jul 01, 2021", "likes": 0}}, {"topLevelComment": {"author": "Aman singh", "text": "sparksession throwing an error- FileNotFoundError: [WinError 2] The system cannot find the file specified", "likes": 0}}, {"topLevelComment": {"author": "Jing Zhou", "text": "love this, post more ))))))))))))))))))))))))))))))))))))", "likes": 0}}, {"topLevelComment": {"author": "johan Rodriguez", "text": "Finished!. But i still want to see the power of this tool.", "likes": 4}}, {"topLevelComment": {"author": "Yash Bhawsar", "text": "@Krish Naik Sir just to clarify at 26:33  I think the Name column min-max decided on the lexicographic order, not by index number.", "likes": 3}}, {"topLevelComment": {"author": "ketchup Party", "text": "A course on Three.js please", "likes": 1}}, {"topLevelComment": {"author": "Cricri", "text": "Yet another excellent offering. Thank you so much.", "likes": 3}}, {"topLevelComment": {"author": "ching-wei yu", "text": "How to run Spark by Docker?", "likes": 1}}, {"topLevelComment": {"author": "Rayyan Amir", "text": "Is this info of pyspark enough to get a relevant job?", "likes": 2}}, {"topLevelComment": {"author": "Praveen Kumar E", "text": "Atlast i found a precious one", "likes": 1}}, {"topLevelComment": {"author": "YG Production", "text": "Dear Mr Beau, thank you so much for amazing courses on this channel. \nI am really grateful how such invaluable courses are available for free.", "likes": 54}, "replies": [{"author": "SAYAN BOSE", "text": "Please thank Mr Krish Naik", "likes": 1}]}, {"topLevelComment": {"author": "Jagmeet Sond", "text": "Freecodecamp please discuss recursion concept very well thanks", "likes": 1}}, {"topLevelComment": {"author": "SanjayGKrish", "text": "It's quite impressive \ud83d\udcab\u2728", "likes": 2}}, {"topLevelComment": {"author": "Nagarajan Nethi", "text": "\ud83e\udd7a\ud83e\udd7a\ud83d\ude4c\ud83d\ude4c\u2763\ufe0f\u2763\ufe0f\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f This is what we need", "likes": 4}}, {"topLevelComment": {"author": "aanchal gujrathi", "text": "Hi, could you please tell me how to skip the header while reading csv file? . option (\"header\",\"False\") is not working", "likes": 1}}, {"topLevelComment": {"author": "Nagarjun P", "text": "You guys are literally reading everyone's mind. Just yesterday I searched for pyspark tutorial and today it's here. Thank you so much. \u2764\ufe0f", "likes": 22}, "replies": [{"author": "Srinivas N", "text": "Not the channel but Youtube is.", "likes": 1}, {"author": "HemaPrasath V", "text": "@uche Hope Recommendation engines pog!?", "likes": 2}, {"author": "uche Hope", "text": "U phone is being tracked.... It's no coincidence.... All our online activities are recorded", "likes": 5}, {"author": "Center Shop Gaming", "text": "Same thing", "likes": 0}]}, {"topLevelComment": {"author": "Ravi Prakash", "text": "Hi Krish, this is very helpful video. I have a question when I try to run pyspark from jupyter notebook I always need to import findspark and initialize the same. But I saw that you were able to directly import pyspark. What could be the problem?", "likes": 6}}, {"topLevelComment": {"author": "Dipak kuchhadiya", "text": "I like it \ud83d\udc4c\ud83c\udffb\n we request you to make video on blockchain programing.", "likes": 3}}, {"topLevelComment": {"author": "Sai Ajay Gundepalli", "text": "Krish naik sir is teaching wow\ud83d\udc4d\ud83d\udc4d", "likes": 1}}, {"topLevelComment": {"author": "Prem Jha", "text": "\ud83d\udd25\ud83d\udd25", "likes": 1}}, {"topLevelComment": {"author": "youtube Curious", "text": "Thalaiva!", "likes": 2}}, {"topLevelComment": {"author": "Raghav Srivastava", "text": "Surprised to see Krish Naik sir here \u2764\ufe0f", "likes": 2}, "replies": [{"author": "Subhajeet Chakraborty", "text": "sameee me tooo \ud83e\udd29", "likes": 1}]}, {"topLevelComment": {"author": "Ronak:Ronu;", "text": "nice to meet you krish sir\ud83d\ude0d", "likes": 1}}, {"topLevelComment": {"author": "VideoZex", "text": "good job", "likes": 1}}, {"topLevelComment": {"author": "ARJUN KUMAR", "text": "Welcome here sir\ud83d\ude4f\ud83d\ude4f", "likes": 1}}, {"topLevelComment": {"author": "Rafi Ahmed", "text": "do u know magi or  something? How  did  um know my  mind", "likes": 0}}, {"topLevelComment": {"author": "Aziz Shifulla", "text": "I want termux full course", "likes": 1}}, {"topLevelComment": {"author": "Candice Russer", "text": "Uploaded at the right time. I was looking for this course. Thank you so much.", "likes": 25}}, {"topLevelComment": {"author": "Kronu", "text": "I want to be a hacker!\nWhere should I start! \ud83e\udd7a\n\nGot scammed 1200$, it's payback time so I need FreeCodeCamp *Sensei* to teach me the Legendary art of Hacking!! \u2764\ufe0f\u2764\ufe0f", "likes": 0}}, {"topLevelComment": {"author": "LAKSHYA PRATAP SIGH", "text": "VERY MUCH HAPPY IN SEEING MY FAVORITE TEACHER COLLABORATING WITH THE FREE CODE CAMP", "likes": 11}}, {"topLevelComment": {"author": "Sharan Phadke", "text": "Biggest crossover : Krish Naik sir teaching for free code camp", "likes": 29}}, {"topLevelComment": {"author": "Sportee Gamer", "text": "Thank you so much to give us these type of courses for free", "likes": 6}}, {"topLevelComment": {"author": "Dave Developed", "text": "Freecodecamp is providing everything to get a high paying job for free", "likes": 0}}, {"topLevelComment": {"author": "__________________________________________________", "text": "Omg krish you are here", "likes": 0}}, {"topLevelComment": {"author": "Mohan Dev", "text": "I didn't expect krish.... Amazingly explained", "likes": 22}}, {"topLevelComment": {"author": "alan henry", "text": "Atlast krish naik sir in freecodecamp\ud83d\ude0d", "likes": 8}}, {"topLevelComment": {"author": "Lucas Mendes", "text": "You guys are awesome! Could you do a CakePHP tutorial please...? I'd love it", "likes": 4}}, {"topLevelComment": {"author": "AM010 DELSON.D", "text": "You have good heart \u2764\ufe0f", "likes": 3}}, {"topLevelComment": {"author": "Aditya Mishra", "text": "Fun Fact: Nobody has watched it yet :)", "likes": 3}}, {"topLevelComment": {"author": "Sumit Dwivedi", "text": "Omg", "likes": 1}}, {"topLevelComment": {"author": "Mathematical Ninja", "text": "I will be watching this tommorow.", "likes": 0}}, {"topLevelComment": {"author": "Sunny Adeshra", "text": "6th comment .... !!!!", "likes": 0}}, {"topLevelComment": {"author": "Vedanth Baliga", "text": "not been this early!", "likes": 0}}, {"topLevelComment": {"author": "pavan raibagi", "text": "I love python,\nnot language......but snake", "likes": 5}, "replies": [{"author": "pavan raibagi", "text": "@Arnav Mehta im lucky to get reply on my first comment", "likes": 0}, {"author": "Arnav Mehta", "text": "I didn't knew that freecodecamp ever replies", "likes": 1}, {"author": "freeCodeCamp.org", "text": "\ud83d\udc0d\ud83d\udc0d\ud83d\udc0d\ud83d\udc0d\ud83d\udc0d\ud83d\udc0d", "likes": 5}, {"author": "pavan raibagi", "text": "@Mukund Jajadiya haha...install sololearn from playstore to learn coding for free", "likes": 0}, {"author": "Mukund Jajadiya", "text": "I also love python,\nLanguage......but not snake\ud83d\ude0a", "likes": 1}]}, {"topLevelComment": {"author": "Cherish Potluri", "text": "Krish Naik on FCC\ud83e\udd2f\ud83d\udd25\ud83d\udd25", "likes": 6}}, {"topLevelComment": {"author": "Anikin Skywalker", "text": "Why are u uploading the good stuff during my exams bro", "likes": 203}, "replies": [{"author": "Erik S", "text": "great comment hahaha!", "likes": 0}, {"author": "ANTON MURSID", "text": "Antonmursid\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\u270c\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\ud83c\uddf8\ud83c\uddec\u270c\ud83d\udc9d\ud83d\udc4c\ud83d\ude4f", "likes": 1}, {"author": "Subramanian Chenniappan", "text": "Can't you watch it later\ud83e\udd23\ud83e\udd23", "likes": 3}, {"author": "Neil Lunavat", "text": "EVEN MY EXAMS GOIN ON", "likes": 5}, {"author": "Settara Pramod", "text": "Xactly", "likes": 4}]}]}