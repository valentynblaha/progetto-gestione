{"video": {"id": "87Gx3U0BDlo", "title": "Beautiful Soup Tutorial - Web Scraping in Python", "description": "The Beautiful Soup module is used for web scraping in Python. Learn how to use the Beautiful Soup and Requests modules in this tutorial. After watching, you will be able to start scraping the web on your own.\n\n\ud83d\udcbbCode: https://github.com/vprusso/youtube_tutorials/tree/master/web_scraping_and_automation/beautiful_soup\n\nTutorial from Vincent Russo of Lucid Programming. Check out his YouTube channel: http://bit.ly/lucidcode\n\n\ud83d\udc26Vincent on Twitter: @captainhamptons\n\n--\n\nLearn to code for free and get a developer job: https://www.freecodecamp.org\n\nRead hundreds of articles on programming: https://medium.freecodecamp.org", "duration": "PT36M55S", "likes": "5612", "views": "297543"}, "comments": [{"topLevelComment": {"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg", "publishedAt": "2019-01-04T15:41:49Z", "author": "LucidProgramming", "text": "Hi everyone, LucidProgramming here, the creator of this video series. If you enjoyed the video, I'm really thrilled to hear that. If you like this type of content, head on over to my channel and consider subscribing for staying up-to-date with similar types of videos. If you have any requests, recommendations, I take these seriously to heart and am constantly trying to improve the content of my channel. Thanks again, and happy coding!", "likes": 255}, "replies": [{"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg.8pgSzqTgLfj9ekG6vlZC5l", "publishedAt": "2022-08-15T10:50:10Z", "author": "LucidProgramming", "text": "@heshan deeyagaha Thank you! If you enjoy this content, please do subscribe to my channel! Cheers!", "likes": 0}, {"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg.8pgSzqTgLfj9ejYxSvJpts", "publishedAt": "2022-08-15T04:15:31Z", "author": "heshan deeyagaha", "text": "This is awesome mate :) Thanks alot, got alot from this small course!", "likes": 0}, {"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg.8pgSzqTgLfj9QoarmgbFDX", "publishedAt": "2021-08-09T07:26:15Z", "author": "LucidProgramming", "text": "@Random Kid Thanks, and yes!", "likes": 0}, {"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg.8pgSzqTgLfj9QnxEQofnPf", "publishedAt": "2021-08-09T01:22:29Z", "author": "Random Kid", "text": "Just subcribed! Also just wondering, are you using Vim? If not what editor/IDE are you using? Thanks in advance!", "likes": 0}, {"id": "UgwTDo1Mu9yIgTJtN-t4AaABAg.8pgSzqTgLfj9Pvb9vYSYNL", "publishedAt": "2021-07-18T04:12:09Z", "author": "cheese cake", "text": "I cant print the part of another attribute", "likes": 0}]}, {"topLevelComment": {"id": "UgwXarA6ejgz7XuH4Ch4AaABAg", "publishedAt": "2023-01-25T20:05:51Z", "author": "TimelessKnight", "text": "18:25 in case of me, first a tag is nonetype  and  it says- urls.append(a_tag.attrs['href'])\r\nAttributeError: 'NoneType' object has no attribute 'attrs'  What can i do to miss first 'a_tag' result?", "likes": 0}}, {"topLevelComment": {"id": "Ugx_sQ2JWZCDDiPdR4t4AaABAg", "publishedAt": "2022-12-16T18:43:42Z", "author": "Br3xin", "text": "Thanks for including info to the other status codes, I really like how you thought ahead", "likes": 0}}, {"topLevelComment": {"id": "UgwJCSqJZnrYzgJ-lpN4AaABAg", "publishedAt": "2022-02-10T10:54:03Z", "author": "Samir EL ZEIN", "text": "you just needed Jupyter to make your life easier with this demo :)", "likes": 1}}, {"topLevelComment": {"id": "UgzJTbP3LcSxRPKtCwR4AaABAg", "publishedAt": "2021-12-08T17:30:06Z", "author": "D Israel", "text": "i really liked the github notes make more vids like that please", "likes": 0}}, {"topLevelComment": {"id": "UgwqV7TT61bV1G78YEx4AaABAg", "publishedAt": "2021-07-20T17:45:22Z", "author": "Sourav Kalal", "text": "Scrapping whatsapp in india is illegal ???", "likes": 0}}, {"topLevelComment": {"id": "UgwY-Gu-yurq3wFpTrB4AaABAg", "publishedAt": "2021-07-18T02:33:24Z", "author": "cheese cake", "text": "This is how The Matrix do code.", "likes": 0}}, {"topLevelComment": {"id": "UgzsENCcuO99BDvGJEh4AaABAg", "publishedAt": "2021-07-08T18:22:49Z", "author": "Rohit Methare", "text": "How do I create a scrapper to track product on Amazon specific category ? Can someone help", "likes": 0}}, {"topLevelComment": {"id": "UgwKosdo8RT-HcFEHjl4AaABAg", "publishedAt": "2021-07-07T15:15:31Z", "author": "Relevel School", "text": "for h2_tag in soup.find_all(\"h2\"):\r\n\ta_tag = h2_tag.find(\"a\")\r\n\tif a_tag == None:\r\n\t\turls.append(\"Empty\")\r\n\t\tcontinue\r\n\turls.append(a_tag.attrs['href'])", "likes": 2}, "replies": [{"id": "UgwKosdo8RT-HcFEHjl4AaABAg.9PVTKsThl_U9YMS_QSAB9U", "publishedAt": "2022-02-12T19:58:17Z", "author": "Tim Nov", "text": "Thanks!", "likes": 0}, {"id": "UgwKosdo8RT-HcFEHjl4AaABAg.9PVTKsThl_U9SdlP5QiA83", "publishedAt": "2021-09-23T19:29:28Z", "author": "TheLastingHD", "text": "absolute legend, thanks", "likes": 0}]}, {"topLevelComment": {"id": "UgzGl_9AvBkZNK6IR4R4AaABAg", "publishedAt": "2021-06-28T14:28:08Z", "author": "Adam Durrant", "text": "I literally fall at the first hurdle with all of these tutorials:\n\nzsh: command not found: pip", "likes": 1}, "replies": [{"id": "UgzGl_9AvBkZNK6IR4R4AaABAg.9P8CkunIVJu9PowgOeTLmc", "publishedAt": "2021-07-15T14:05:33Z", "author": "PAUL BLART", "text": "do you have python installed?", "likes": 0}]}, {"topLevelComment": {"id": "UgzPWOv4ciGzvwNYLwh4AaABAg", "publishedAt": "2021-06-22T08:29:17Z", "author": "Seda Karanfil", "text": "Hi, thanks for the tutorial. I am new to Python and almost totally ignorant. I downloaded Python 3.9.5 and trying to use IDLE for web scraping for a long while. You have this tab down there which starts with \"NORMAL\" and where you enter soup_and_requests.py. I don't have that part. Since I am totally ignorant, I do not know how to have that tab at the very bottom. I would be so grateful if you can answer my stupid question. Thanks so much.", "likes": 1}, "replies": [{"id": "UgzPWOv4ciGzvwNYLwh4AaABAg.9Ot6vSsgdqO9Ps8yQfviaI", "publishedAt": "2021-07-16T19:59:19Z", "author": "Movies Night", "text": "Hi Seda. He is using Vim Editor. If you are new to python, i recommend you to install \"anaconda\". Currently i am using spyder editor in anaconda. It is beginner friendly :) LMK if you good with this", "likes": 0}]}, {"topLevelComment": {"id": "Ugy3smkMu3krsMt8UdR4AaABAg", "publishedAt": "2021-06-03T13:09:19Z", "author": "Mike Waters", "text": "Super helpful that unlike other videos, goes into depth how to actually extract different things. Thank you.", "likes": 0}}, {"topLevelComment": {"id": "Ugw-yB9Dd6XyUoLCS0V4AaABAg", "publishedAt": "2021-04-30T14:47:42Z", "author": "Komoni Efe", "text": "This has been helpful", "likes": 0}}, {"topLevelComment": {"id": "UgxGzUVPKfo4k1wTZFR4AaABAg", "publishedAt": "2021-03-18T05:51:23Z", "author": "You are IN-CONTROL", "text": "I am getting \"AttributeError: 'NoneType' object has no attribute 'attrs'\" on urls.append(a_tag.attrs['href'])", "likes": 0}, "replies": [{"id": "UgxGzUVPKfo4k1wTZFR4AaABAg.9L0dXt5agpq9L2tMuoV7Hp", "publishedAt": "2021-03-19T02:48:10Z", "author": "Nathan Hackleman", "text": "I've run into the same issue! I've tried copying the code from the GitHub link but no luck.", "likes": 0}]}, {"topLevelComment": {"id": "UgyxU9YBCseOD_eCKcZ4AaABAg", "publishedAt": "2021-03-10T21:20:52Z", "author": "Vapen_Hem", "text": "Doesn't work", "likes": 1}}, {"topLevelComment": {"id": "UgyVdByHMQckyV7Wiwt4AaABAg", "publishedAt": "2021-03-05T12:31:33Z", "author": "Trends For You", "text": "urls.append(a_tag.attrs['href'])\r\nAttributeError: 'NoneType' object has no attribute 'attrs'\n\nIt is giving me this error while appending my list. Anyone can help ???", "likes": 1}, "replies": [{"id": "UgyVdByHMQckyV7Wiwt4AaABAg.9KVt-O62gDt9Lt9J0iEsDy", "publishedAt": "2021-04-08T19:15:56Z", "author": "Ahsen Zafar", "text": "I found the below solution online\n\nfor h2_tag in soup.find_all('h2'):\r\n\ta_tag = h2_tag.find('a')\r\n\tif a_tag is not None and 'href' in a_tag.attrs:\r\n\t\turl.append(a_tag.attrs['href'])", "likes": 2}, {"id": "UgyVdByHMQckyV7Wiwt4AaABAg.9KVt-O62gDt9KWJXTbMaL6", "publishedAt": "2021-03-05T16:32:07Z", "author": "Tinashe Chinyati", "text": "Also having the same problem", "likes": 1}]}, {"topLevelComment": {"id": "UgwfwFAxgrtYAH0qX3B4AaABAg", "publishedAt": "2021-03-02T10:25:50Z", "author": "CodeCSE", "text": "if you are a mac user, then use pip3 instead of pip. for mac pip3", "likes": 0}}, {"topLevelComment": {"id": "Ugw5jcUVeAaddHzcMkx4AaABAg", "publishedAt": "2021-02-25T10:24:02Z", "author": "Ke H", "text": "It doesn't return any links containing 'about'even with the correct code..any ideas?", "likes": 0}}, {"topLevelComment": {"id": "Ugx9ZClgo8Fsz9qp2nB4AaABAg", "publishedAt": "2021-02-17T18:16:51Z", "author": "Tobor Two", "text": "searching with tags \"div\" doesn't work with link.text. I don't think it recognizes div to be text. what do I need to do instead to find key words?", "likes": 0}}, {"topLevelComment": {"id": "UgwX3XWQJyXVSMVVu194AaABAg", "publishedAt": "2021-02-15T21:33:45Z", "author": "Bukszpryt", "text": "apparenty in the whitehouse website, first <a> in <h2> is None, so to avoid error, before appending a_tag to urls, make sure a_tag is not None\n    ...\n    a_tag = ...\n    if a_tag is not None:\n        urls.append(a_tag.attrs['href'])", "likes": 2}, "replies": [{"id": "UgwX3XWQJyXVSMVVu194AaABAg.9JnVjttNoew9PVStqNerKb", "publishedAt": "2021-07-07T15:11:41Z", "author": "Relevel School", "text": "this was very helpful", "likes": 1}, {"id": "UgwX3XWQJyXVSMVVu194AaABAg.9JnVjttNoew9KIiBMymAwq", "publishedAt": "2021-02-28T09:46:56Z", "author": "Ryan C.", "text": "this was giving me an error and you solved it, thank you :)", "likes": 1}]}, {"topLevelComment": {"id": "UgybZdZJGCCi8Tf36XZ4AaABAg", "publishedAt": "2021-02-04T07:02:53Z", "author": "WHO KNOWS?", "text": "this dude lowkey sounds like charlie/penguinz0", "likes": 0}}, {"topLevelComment": {"id": "UgwTlUyyWcMzdvQ90Y54AaABAg", "publishedAt": "2021-01-06T07:51:04Z", "author": "Apoorv", "text": "at 18:18 run progam i am getting below error, pls help\n line 13, in <module>\n urls.append(a_tag.attrs['href'])\r\n\nAttributeError: 'NoneType' object has no attribute 'attrs'", "likes": 9}, "replies": [{"id": "UgwTlUyyWcMzdvQ90Y54AaABAg.9IA1nOM_6XU9_n4OQeC85o", "publishedAt": "2022-04-14T06:28:27Z", "author": "Taimur", "text": "@Michael Peters Thank ye!", "likes": 0}, {"id": "UgwTlUyyWcMzdvQ90Y54AaABAg.9IA1nOM_6XU9JnW1ebkTWo", "publishedAt": "2021-02-15T21:36:19Z", "author": "Bukszpryt", "text": "before appending a_tag to urls, you can make sure it is not None.\n\nif a_tag is not None:\n        urls.append(a_tag.attrs['href'])", "likes": 1}, {"id": "UgwTlUyyWcMzdvQ90Y54AaABAg.9IA1nOM_6XU9JlFbLYPMS4", "publishedAt": "2021-02-15T00:34:17Z", "author": "James Mitchell", "text": "@Pei Wang I actually found that Michael's code was hit or miss and so tried to identify the specific h2 tag and came up with this\n\nurls =[ ]\r\nfor x in soup.findAll('h2',{'class':'news-item__title-container'}):\r\n    links = x.find_all('a')\r\n    urls.append(links)\r\n    \r\nprint(urls)\n\nUnfortunately it errors out when I try to pull the href attributes so....not a perfect solution, but appears consistent.", "likes": 0}, {"id": "UgwTlUyyWcMzdvQ90Y54AaABAg.9IA1nOM_6XU9J7K3HmcjTl", "publishedAt": "2021-01-30T03:04:20Z", "author": "Pei Wang", "text": "@Michael Peters Thanks Michael. Your method is helpful :)", "likes": 0}, {"id": "UgwTlUyyWcMzdvQ90Y54AaABAg.9IA1nOM_6XU9Iy9KwEadWT", "publishedAt": "2021-01-26T04:18:13Z", "author": "Michael Peters", "text": "for h2_tag in soup.find_all(\"h2\"):\r\n\ta_tag = h2_tag.find(\"a\")\r\n\tif a_tag == None:\r\n\t\turls.append(\"Empty\")\r\n\t\tcontinue\r\n\turls.append(a_tag.attrs['href'])\n\ntry replacing with the above,\n\ni did a search through the website source since this video was published, and theres some h2 tags for non articles. You should be getting a \"NoneType\" return for when you're calling the attrs method. Its because there is no a tag under some h2 tags, and bs4 is returning a NoneType object.\n\nI've thrown in an error check to see is the a_tag variable returns a None object, continue through the loop", "likes": 7}]}, {"topLevelComment": {"id": "UgzrwUyRJFrqhbN2nP54AaABAg", "publishedAt": "2021-01-04T17:19:34Z", "author": "Leo Bozkir", "text": "3:43, 4:31, 5:18, 7:51\nAmazing vid!", "likes": 1}}, {"topLevelComment": {"id": "Ugzo3q10dv04nVbbLkZ4AaABAg", "publishedAt": "2020-12-26T15:16:35Z", "author": "pawan mishra", "text": "he sounds like someone caught cold, btw it was very helpful.", "likes": 1}}, {"topLevelComment": {"id": "UgyRlmnk7JUDCPFJQPx4AaABAg", "publishedAt": "2020-12-21T22:58:10Z", "author": "God Plaz", "text": "For some reason, it says ModuleNotFoundError when I try to type in \"from bs4 import BeautifulSoup\"", "likes": 0}, "replies": [{"id": "UgyRlmnk7JUDCPFJQPx4AaABAg.9HYSt8XZv2W9HZsJUD-Swn", "publishedAt": "2020-12-22T12:08:19Z", "author": "Martin Wong", "text": "run in ur terminal -> pip install bs4", "likes": 0}]}, {"topLevelComment": {"id": "UgzVWfeoWkV4TraNv1V4AaABAg", "publishedAt": "2020-12-19T21:59:27Z", "author": "Wilson Yu", "text": "I used to be a chef that makes delicious soup, after seeing this I became a programmer that makes beautiful soups.", "likes": 0}}, {"topLevelComment": {"id": "Ugzevz-uZHl8F-Ri7214AaABAg", "publishedAt": "2020-12-12T22:17:05Z", "author": "Kamilee Pascual", "text": "very helpful but I kept falling asleep through it...", "likes": 0}}, {"topLevelComment": {"id": "UgxbrbKwwwcNIoTyj5R4AaABAg", "publishedAt": "2020-12-11T14:44:59Z", "author": "Ryan", "text": "What editor is this? Makes moving around seem so easy", "likes": 0}}, {"topLevelComment": {"id": "UgzQij-x6srSW1ndOaV4AaABAg", "publishedAt": "2020-12-10T10:53:20Z", "author": "Mian Ali", "text": "We are providing services Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. Usually, such software programs simulate human exploration of the World Wide Web by either implementing low-level Hypertext Transfer Protocol (HTTP), or embedding a fully-fledged web browser, such as Internet Explorer, Google Chrome and Mozilla Firefox\n\n\nContact Us\nhttps://aitomation.com/data-web-scraping/", "likes": 0}}, {"topLevelComment": {"id": "UgwRI6HSHvoKD838qtJ4AaABAg", "publishedAt": "2020-12-02T12:30:15Z", "author": "RATAN AGARWAL", "text": "nice", "likes": 0}}, {"topLevelComment": {"id": "UgxLKsTnRTlNkrnKZ0h4AaABAg", "publishedAt": "2020-11-29T12:11:24Z", "author": "Biff Bifford", "text": "I would like to say that my first computer was an Osborne 8088 (cira, 1981). I begin programming in the mid-1980s in Dbase, Pascal, and C as a hobby. In the 1980s, we would log into bulletin boards that were someone's computer, which had games and other things to trade and share. This was my internet when I was a kid. I just want to say thank you, FreeCodeCamp, for bringing the joy back into computer programming for me with such rich and insightful content.", "likes": 1}}, {"topLevelComment": {"id": "UgxQv4nhKH0PORvJnjZ4AaABAg", "publishedAt": "2020-11-20T03:44:09Z", "author": "octo1", "text": "For anyone getting the  \nAttributeError: 'NoneType' object has no attribute 'attrs'\nerror - the list returns two items that are empty for some reason. \n\nUse \nif a_tag is not None:\nbefore you append to urls", "likes": 15}, "replies": [{"id": "UgxQv4nhKH0PORvJnjZ4AaABAg.9GG_AlrIHIZ9PsBBmSsBDB", "publishedAt": "2021-07-16T20:18:45Z", "author": "Movies Night", "text": "@Randy Miskuski u missed indentation. This works\nurls = []\r\nfor h2_tag in soup.find_all(\"h2\"):\r\n    a_tag = h2_tag.find('a')\n    if a_tag is not None:\n        urls.append(a_tag.attrs['href'])", "likes": 1}, {"id": "UgxQv4nhKH0PORvJnjZ4AaABAg.9GG_AlrIHIZ9M2Q2w9TfWI", "publishedAt": "2021-04-12T18:54:42Z", "author": "Randy Miskuski", "text": "urls = []\r\nfor h2_tag in soup.find_all(\"h2\"):\r\n    a_tag = h2_tag.find('a')\r\nif a_tag is not None:\r\n    urls.append(a_tag.attrs['href'])\n\nIs this what you mean? (My print(urls) outcome is an empty list)", "likes": 0}, {"id": "UgxQv4nhKH0PORvJnjZ4AaABAg.9GG_AlrIHIZ9L0duOjGxa3", "publishedAt": "2021-03-18T05:54:35Z", "author": "You are IN-CONTROL", "text": "This works. Thank you!", "likes": 0}, {"id": "UgxQv4nhKH0PORvJnjZ4AaABAg.9GG_AlrIHIZ9KB1CfSLJDa", "publishedAt": "2021-02-25T10:07:57Z", "author": "Ke H", "text": "Why?", "likes": 0}, {"id": "UgxQv4nhKH0PORvJnjZ4AaABAg.9GG_AlrIHIZ9GwV7CFd7mR", "publishedAt": "2020-12-06T19:47:19Z", "author": "Donovan", "text": "Thank you!", "likes": 0}]}, {"topLevelComment": {"id": "Ugy4pdHASHwg3cddpV94AaABAg", "publishedAt": "2020-11-17T11:35:11Z", "author": "Another Look", "text": "I am only 11 minutes in, and this is one of the clearest explanations of the data extraction syntax that I have come across so far.", "likes": 4}}, {"topLevelComment": {"id": "UgwGEc96JxWIPijctN14AaABAg", "publishedAt": "2020-11-15T12:55:19Z", "author": "Mentor 101", "text": "If any one getting lxml error while running the code just do:\npip install lxml", "likes": 0}}, {"topLevelComment": {"id": "UgzVLst44VTvOSj25V94AaABAg", "publishedAt": "2020-11-05T23:55:55Z", "author": "Mike W", "text": "Boom! \nAt 16:45 you find the a tags inside the h2 tags! That's what I needed. \nVery happy! Thank you!", "likes": 0}}, {"topLevelComment": {"id": "Ugy7-eG6XyW5xQ_oMpZ4AaABAg", "publishedAt": "2020-11-05T18:48:25Z", "author": "Charles Lau", "text": "Great tutorial. Very clear and concise", "likes": 0}}, {"topLevelComment": {"id": "UgyYkWhgq19RVl4gF8h4AaABAg", "publishedAt": "2020-09-21T17:01:28Z", "author": "Kadir Durak", "text": "Guys, I have a problem with this and would appreciate any help:\nWhenever I attempt to run this program I get this error. I have followed all the steps, typed it correctly, installed it into the same folder as my program and python, installed all the other software in the comments section (lxml, parser) but I still get this error, if I have missed anything please tell me as it is getting very annoying now. The error is \"ModuleNotFoundError: No module named 'requests'\"", "likes": 2}, "replies": [{"id": "UgyYkWhgq19RVl4gF8h4AaABAg.9DsVj-UiKRI9EcKFndaBLH", "publishedAt": "2020-10-10T06:44:46Z", "author": "Radu", "text": "pip install requests should do the job, is it a pylint error?", "likes": 1}, {"id": "UgyYkWhgq19RVl4gF8h4AaABAg.9DsVj-UiKRI9EBxFfoo68F", "publishedAt": "2020-09-29T15:35:32Z", "author": "aka011", "text": "try : pip list , to see if the requests module was installed and if not try install it again", "likes": 0}]}, {"topLevelComment": {"id": "Ugxz12S01Qq2GktUHit4AaABAg", "publishedAt": "2020-08-29T11:08:39Z", "author": "Project Vibe", "text": "if you are getting an error and windows 32bit then use html.parser instead of lxml", "likes": 1}}, {"topLevelComment": {"id": "UgzDzcy6GddVzQFE_y94AaABAg", "publishedAt": "2020-08-24T13:48:19Z", "author": "A ", "text": "great video!!", "likes": 0}}, {"topLevelComment": {"id": "UgxXwYNY0stWuF3Xh7F4AaABAg", "publishedAt": "2020-08-14T22:00:53Z", "author": "Abdul Raheem", "text": "Thanks, it really helps me to understand the uses of beautifulsoup functions.", "likes": 1}}, {"topLevelComment": {"id": "UgwAQt89yl0xO06SP514AaABAg", "publishedAt": "2020-08-07T08:18:59Z", "author": "it's 6884", "text": "Someone here has a big time cold, huh?", "likes": 0}}, {"topLevelComment": {"id": "UgwMAMght6I5Ob7FHQN4AaABAg", "publishedAt": "2020-08-04T12:37:19Z", "author": "pooja kumari", "text": "How can we fetch specific tags for example- address I want to fetch from multiple files I have in .txt format.\r\nplzzzz provide solution ....\r\nfor one file I can fetch but how can I fetch for multiples files:-\r\naddresses = []\r\nwith open(\"/rawhtml/greerwilsonchapel.com_executives_contact_us.txt\") as fp:\r\n    soup = BeautifulSoup(fp)\r\n    address = soup.find('div',class_=\"locator-titles\").get_text().rstrip('\\n').split('\\n')\r\n    addresses.append(address)\r\n    print(addresses)", "likes": 0}}, {"topLevelComment": {"id": "UgyQGwzDIPI1y-QXV754AaABAg", "publishedAt": "2020-07-31T15:02:36Z", "author": "Aaron Rumley", "text": "this is an awesome tutorial, thank you! very clear and concise, but with lots of content!", "likes": 1}}, {"topLevelComment": {"id": "UgzhXGOytyR1mH1UIuB4AaABAg", "publishedAt": "2020-07-27T23:49:08Z", "author": "Ali Alavi", "text": "Thanks for the video. Could you explain how to scrap within parents or siblings of a tag?", "likes": 0}}, {"topLevelComment": {"id": "UgxZhQ_vNdOemtulHJd4AaABAg", "publishedAt": "2020-07-10T10:49:17Z", "author": "vinay vyas", "text": "are u using vim editor!!?", "likes": 0}}, {"topLevelComment": {"id": "UgzQ5rTPXO_YbM-eDiJ4AaABAg", "publishedAt": "2020-06-17T08:13:04Z", "author": "Mohammed AFAOUNODDEN AHMED", "text": "Amazing Content !!!  i just want to know at what case will we use API's to Scrape Data from websites ?", "likes": 0}}, {"topLevelComment": {"id": "UgxyKg0g8lCPdqj-sIN4AaABAg", "publishedAt": "2020-06-16T14:43:04Z", "author": "Michael H", "text": "This is a stand-out tutorial, thanks!", "likes": 4}}, {"topLevelComment": {"id": "Ugz3utpzJAvzK73dpHV4AaABAg", "publishedAt": "2020-06-06T19:15:16Z", "author": "robin vermillion", "text": "they changed the website the scraping doesnt work anymore.", "likes": 0}}, {"topLevelComment": {"id": "UgxEmTvfeGV1fafYGbN4AaABAg", "publishedAt": "2020-06-06T15:45:24Z", "author": "vijay vittal", "text": "What ide is this?", "likes": 0}}, {"topLevelComment": {"id": "UgwFepG_B8HdwKLb8op4AaABAg", "publishedAt": "2020-06-03T16:03:27Z", "author": "Patricio Reese", "text": "what text editor is that?", "likes": 0}}, {"topLevelComment": {"id": "Ugy6YYPkgKjshMGndI54AaABAg", "publishedAt": "2020-05-28T13:45:39Z", "author": "Sahara Manson 1", "text": "Awesome", "likes": 0}}, {"topLevelComment": {"id": "UgyhhhIS5-frPhyov-l4AaABAg", "publishedAt": "2020-05-27T10:20:59Z", "author": "Mehmet Ali Karada\u011f", "text": "Can i scroll down and put input into boxes? For ex. When you scroll down on instagram followers page, you get new elements. I can do this with selenium but it is not stable.", "likes": 0}}, {"topLevelComment": {"id": "UgxyPW1jASzK99IJ4Bl4AaABAg", "publishedAt": "2020-05-25T12:37:02Z", "author": "Dean 007", "text": "Worst video on bs4", "likes": 1}}, {"topLevelComment": {"id": "Ugw5CjUUj8cwMBJo9Dx4AaABAg", "publishedAt": "2020-05-22T06:27:06Z", "author": "Yoruba Ambassador", "text": "What's the difference between tag.text, tag.content and tag.string? \n\nAlso, what's the difference between tag.attrs['href'] and tag['href'] ??", "likes": 0}}, {"topLevelComment": {"id": "Ugx9-g9kgCEid_cbmXt4AaABAg", "publishedAt": "2020-05-19T03:03:37Z", "author": "ITZ CHRYSLER", "text": "what a lazy way to teach bro", "likes": 0}}, {"topLevelComment": {"id": "Ugzoq3un0qYPuAiELzZ4AaABAg", "publishedAt": "2020-05-16T13:16:58Z", "author": "Reg Chand", "text": "well explained :)", "likes": 0}}, {"topLevelComment": {"id": "UgxMROn7cUFtG8BoLzV4AaABAg", "publishedAt": "2020-05-14T17:02:34Z", "author": "Sabah Samareen", "text": "<html>\r\n <body>\r\n  <p>\r\n   html_doc\r\n  </p>\r\n </body>\r\n</html>\nthis is the o/p i got when dealing with objects of beautifulsoup  i also wanted to know whether the content of the variable html_doc was pasted from the web page using 'view page source'", "likes": 1}}, {"topLevelComment": {"id": "UgzG2_vm9JLNBI8ZPd94AaABAg", "publishedAt": "2020-05-13T12:32:17Z", "author": "W Chen", "text": "Thanks for the video!\nBy the way if we don't install \"pip install lxml\", change to below \"html.parser\" and it won't get error :-)\nsoup = BeautifulSoup(scr,\"html.parser\")", "likes": 24}, "replies": [{"id": "UgzG2_vm9JLNBI8ZPd94AaABAg.98ahmpFJGB79M4fx2dEYHu", "publishedAt": "2021-04-13T16:00:47Z", "author": "Hector Kolind", "text": "Glad a read the comments before starting.", "likes": 0}, {"id": "UgzG2_vm9JLNBI8ZPd94AaABAg.98ahmpFJGB79IDm-qzeKTs", "publishedAt": "2021-01-07T18:41:19Z", "author": "Nicolas Szernek", "text": "Savior! Thanks a lot!", "likes": 0}, {"id": "UgzG2_vm9JLNBI8ZPd94AaABAg.98ahmpFJGB79AY9jQa1cVe", "publishedAt": "2020-06-30T20:31:03Z", "author": "dawg", "text": "Thanks a lot!", "likes": 0}, {"id": "UgzG2_vm9JLNBI8ZPd94AaABAg.98ahmpFJGB79A1IXbXmzGz", "publishedAt": "2020-06-18T02:13:00Z", "author": "Jae En", "text": "Thanks for that \"pip instal lxml\", I had that problem, you solved it.  Cheers!", "likes": 1}]}, {"topLevelComment": {"id": "UgwKu75Zs68Iq_Xt2Pp4AaABAg", "publishedAt": "2020-05-13T07:02:46Z", "author": "Gwanghyeon Gim", "text": "Thank you so much for your work. I've never seen any python lecturer who dealt with Vim before! Vim is super fast and works great with python file. I really enjoyed your bs4 tutorial. Keep the torch up high!", "likes": 5}}, {"topLevelComment": {"id": "Ugz42pIXxNYd8PabeU94AaABAg", "publishedAt": "2020-05-13T04:00:20Z", "author": "Gwanghyeon Gim", "text": "Why use 'pip install bs4' for installing beautifulsoup4?", "likes": 0}}, {"topLevelComment": {"id": "Ugz3BRNMVphjP-JxJKR4AaABAg", "publishedAt": "2020-05-09T05:22:04Z", "author": "Tech Notes", "text": "Thank You for this video. Most of the content of this video I enjoyed of the work in VIM like with IDE. Always beautiful...", "likes": 0}}, {"topLevelComment": {"id": "UgxUrWzJwSCf-Dmb9DB4AaABAg", "publishedAt": "2020-05-05T17:16:46Z", "author": "Martin Kaspar", "text": "hi there great rundown - great stuff - i like it very much _ btw. do you offer the code somewhere !?", "likes": 0}}, {"topLevelComment": {"id": "UgxeJ5tRgOe-M1284FV4AaABAg", "publishedAt": "2020-04-29T01:03:10Z", "author": "Bill Barron", "text": "What do you suggest if we run into the 404 error code you mentioned in the beginning?\nI made sure the url was correct but am getting that error. Any advise?", "likes": 0}}, {"topLevelComment": {"id": "Ugzg-l01wCFgzBK39cB4AaABAg", "publishedAt": "2020-04-26T17:02:59Z", "author": "Suraj thapa", "text": "Thank you so much for this intro video!!! :)", "likes": 0}}, {"topLevelComment": {"id": "Ugx-GN2qR3_i5zIZOpB4AaABAg", "publishedAt": "2020-04-02T09:31:30Z", "author": "Duwal Kreation", "text": "this guy taught us well but he didnt clean his nose before recording the video", "likes": 1}}, {"topLevelComment": {"id": "Ugz2CyWTMWlhGtiGKWx4AaABAg", "publishedAt": "2020-04-01T16:32:10Z", "author": "Peter Loizou", "text": "Clear instruction, good pace. Thanks.", "likes": 0}}, {"topLevelComment": {"id": "UgwxTRKA-KMi9g7fSud4AaABAg", "publishedAt": "2020-03-25T09:04:39Z", "author": "Vila Maker", "text": "nice video. congrats.", "likes": 0}}, {"topLevelComment": {"id": "Ugw6XnaOEtx4OIeIQlR4AaABAg", "publishedAt": "2020-03-22T13:54:51Z", "author": "Tom\u00e1\u0161 Hor\u00e1\u010dek", "text": "Did you have a cold?", "likes": 1}}, {"topLevelComment": {"id": "UgyGhG1k4BmVu2ELcKt4AaABAg", "publishedAt": "2020-03-22T02:09:25Z", "author": "Richard HARYONO", "text": "Hello, the 3 files from your GitHub, does not really corresponds to your Youtube Video above, any idea? What I read in your GitHub's py files - are all seems HTML coded scripts. Something's wrong I think... What have I missed? Thanks.", "likes": 0}, "replies": [{"id": "UgyGhG1k4BmVu2ELcKt4AaABAg.96UhAMMkgon96YczENSwqC", "publishedAt": "2020-03-23T14:49:46Z", "author": "Richard HARYONO", "text": "I found the answers :) Silly... Thanks.", "likes": 0}]}, {"topLevelComment": {"id": "UgwpN9sbj0w9nBIVhYV4AaABAg", "publishedAt": "2020-03-09T06:23:58Z", "author": "Sridhar SG", "text": "Those who are getting this error:\nbs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n\n\nuse \"soup = BeautifulSoup(src, \"html5lib\")\" instead of \"soup = BeautifulSoup(src, \"lxml\")\"", "likes": 3}, "replies": [{"id": "UgwpN9sbj0w9nBIVhYV4AaABAg.95yfyMSNbnM9652sre7MjS", "publishedAt": "2020-03-12T03:07:37Z", "author": "heyou", "text": "I too had to figure that out the hard way.  What is wrong with lxml, old and dead???", "likes": 0}]}, {"topLevelComment": {"id": "UgwkSN9dWudvbp6WSS94AaABAg", "publishedAt": "2020-03-02T05:50:06Z", "author": "The Almost Cool Kid", "text": "Man, I love it when it just prints \"[ ]\"", "likes": 38}, "replies": [{"id": "UgwkSN9dWudvbp6WSS94AaABAg.95gaXBvuU8C98X9quQNbxK", "publishedAt": "2020-05-11T18:10:02Z", "author": "Xiao Yao Pai Polska", "text": "@The Almost Cool Kid Thanks, I found on stackoverflow exact problems related to this course :) Happy coding in java! :)", "likes": 0}, {"id": "UgwkSN9dWudvbp6WSS94AaABAg.95gaXBvuU8C98X8gw_a1-Q", "publishedAt": "2020-05-11T17:59:57Z", "author": "The Almost Cool Kid", "text": "@Xiao Yao Pai Polska It prints that when it doesn't find anything in the class you're trying to reference", "likes": 4}, {"id": "UgwkSN9dWudvbp6WSS94AaABAg.95gaXBvuU8C98X8dfvSYB3", "publishedAt": "2020-05-11T17:59:30Z", "author": "The Almost Cool Kid", "text": "@Xiao Yao Pai Polska nope, I gave up and started learning java", "likes": 6}, {"id": "UgwkSN9dWudvbp6WSS94AaABAg.95gaXBvuU8C98WC0DekXqv", "publishedAt": "2020-05-11T09:09:41Z", "author": "Xiao Yao Pai Polska", "text": "Did you figure it out how to fix it?", "likes": 0}, {"id": "UgwkSN9dWudvbp6WSS94AaABAg.95gaXBvuU8C98CDFpDU8EY", "publishedAt": "2020-05-03T14:55:45Z", "author": "tdreamgmail", "text": "lol", "likes": 0}]}, {"topLevelComment": {"id": "UgwcNUY72tbJJSp7gCJ4AaABAg", "publishedAt": "2020-03-01T05:27:58Z", "author": "Rahdin Zaman", "text": "How can you call attributes like .status_code on result? Does the first line instantiate result as a requests object or something like that?", "likes": 0}}, {"topLevelComment": {"id": "UgwPub0KP8XFEMPq6s94AaABAg", "publishedAt": "2020-03-01T05:27:01Z", "author": "Syn", "text": "Comment", "likes": 0}}, {"topLevelComment": {"id": "UgyaG6i-paPAviqnJ2N4AaABAg", "publishedAt": "2020-02-21T07:21:07Z", "author": "binaya batas", "text": "great video. is there any video for selenium?", "likes": 0}}, {"topLevelComment": {"id": "UgzOiqnoncaNkOCaaKd4AaABAg", "publishedAt": "2020-02-02T00:48:47Z", "author": "Joe Gancher", "text": "I'm missing something here. What are you typing in ? A Unix shell ? A Dos Command environment. I'm not getting this. i type in this stuff, at a Command line prompt and get \"'pip' is not recognized as an internal or external command,\r operable program or batch file.\"", "likes": 1}}, {"topLevelComment": {"id": "UgzxJngST1v0-mvLNUp4AaABAg", "publishedAt": "2020-01-24T21:23:56Z", "author": "martin", "text": "Brillant video simple and straight to the point. More please", "likes": 0}}, {"topLevelComment": {"id": "UgzpxNvPrX7RK1tJ6uV4AaABAg", "publishedAt": "2020-01-23T20:41:22Z", "author": "zach wallen", "text": "I've watched multiple web scraping tutorials and this is definitely the most descriptive and most helpful for actually learning how to do this on your own. Thanks!", "likes": 4}}, {"topLevelComment": {"id": "UgxqBsT_p1d0uCLme0h4AaABAg", "publishedAt": "2020-01-21T20:47:57Z", "author": "mirza salman", "text": "what is the name of the editor...?", "likes": 0}, "replies": [{"id": "UgxqBsT_p1d0uCLme0h4AaABAg.943cgFuv6ax98_qtBoibBS", "publishedAt": "2020-05-13T04:32:33Z", "author": "Gwanghyeon Gim", "text": "It's Vim. It is very likely that it's already installed in your computer.", "likes": 0}]}, {"topLevelComment": {"id": "UgwqxlACVTysJ2VTFBR4AaABAg", "publishedAt": "2020-01-04T22:13:38Z", "author": "MEME SQUAD", "text": "Exactly 1 year later still thanks ;)", "likes": 2}}, {"topLevelComment": {"id": "UgwMLUdWDKxhnge9yAt4AaABAg", "publishedAt": "2019-12-20T16:25:31Z", "author": "Ishan Kashyap", "text": "Very Informative video.. thank u, sir.", "likes": 0}}, {"topLevelComment": {"id": "Ugx8HHZva8YT0nWn9l14AaABAg", "publishedAt": "2019-12-13T23:46:24Z", "author": "Forest River", "text": "yeah lets make multiple requests to the white houses website hahahahaha", "likes": 0}}, {"topLevelComment": {"id": "UgwkaMqZa-7FnTITStF4AaABAg", "publishedAt": "2019-11-30T13:21:47Z", "author": "gayathri sankar", "text": "Good tutorial .  I thought I was watching with 1.25x speed but I wasn't", "likes": 11}}, {"topLevelComment": {"id": "Ugz6lPRdrt71e2XSrLR4AaABAg", "publishedAt": "2019-11-24T10:52:52Z", "author": "MrBrightsideTK", "text": "Good tutorial... and nice editor. Feel like back in 90's using MS DOS.", "likes": 0}, "replies": [{"id": "Ugz6lPRdrt71e2XSrLR4AaABAg.91iDTzcQJpf923zECjHyRk", "publishedAt": "2019-12-03T07:02:10Z", "author": "saurabh sharma.", "text": "it's vim", "likes": 0}]}, {"topLevelComment": {"id": "Ugw9EATkKVd5gC6ZjC14AaABAg", "publishedAt": "2019-10-25T14:07:34Z", "author": "Naveen D", "text": "Thank you for the lecturing about web scraping using python", "likes": 0}}, {"topLevelComment": {"id": "UgztGjf3DOM6iEftsI94AaABAg", "publishedAt": "2019-10-02T15:10:24Z", "author": "Hitman 47", "text": "could you put the web link scraped to description?", "likes": 0}}, {"topLevelComment": {"id": "Ugy7B3gXpOCISCfqIxh4AaABAg", "publishedAt": "2019-09-22T19:03:35Z", "author": "Typhon", "text": "pip does not work for command prompt idk why", "likes": 0}}, {"topLevelComment": {"id": "Ugx_2qAS4E8Tk3gc0q54AaABAg", "publishedAt": "2019-08-20T18:27:56Z", "author": "Sachin Negi", "text": "clean and straightforward", "likes": 0}}, {"topLevelComment": {"id": "UgyFUtHUDRrK76fjAvp4AaABAg", "publishedAt": "2019-08-01T07:09:12Z", "author": "Sakib", "text": "what kind of project is this? i cant  create Beatifulsoup class in a py file", "likes": 0}}, {"topLevelComment": {"id": "Ugz9IpypqiYApG96O3h4AaABAg", "publishedAt": "2019-07-28T20:38:02Z", "author": "Wizard OfOz", "text": "when typing import requests i get \"import requests is not recognized as a cmdlet \"", "likes": 1}, "replies": [{"id": "Ugz9IpypqiYApG96O3h4AaABAg.8xwqqpAvO4J90gtMwgPXBp", "publishedAt": "2019-10-30T01:57:45Z", "author": "rafael dias", "text": "Reinstall your python and mak option  to add python to the PATH of you system variables.", "likes": 0}, {"id": "Ugz9IpypqiYApG96O3h4AaABAg.8xwqqpAvO4J90ADFZ2WoMx", "publishedAt": "2019-10-17T00:06:04Z", "author": "Mike C", "text": "check your paths.", "likes": 0}]}, {"topLevelComment": {"id": "Ugxs2Tmz12EQlhgVi_N4AaABAg", "publishedAt": "2019-07-26T14:34:14Z", "author": "Fatima Zahra", "text": "plz how can i stock this data in database phpmyadmin???", "likes": 0}}, {"topLevelComment": {"id": "Ugwpp7Ti5dfyrDsDItt4AaABAg", "publishedAt": "2019-07-25T20:25:04Z", "author": "nick brownrigg", "text": "took me 3 hours just to figure out i need to add the Lib/site-packages in visual studios just to import requests.  what a drag. idk how i even ended up on this page", "likes": 24}, "replies": [{"id": "Ugwpp7Ti5dfyrDsDItt4AaABAg.8xp5zGH8HWe9AlH21BDga3", "publishedAt": "2020-07-06T08:04:16Z", "author": "Faisal Nazik", "text": "me too . but not 3 hours", "likes": 0}]}, {"topLevelComment": {"id": "UgyUb2WoyZY39gFFG0x4AaABAg", "publishedAt": "2019-07-23T16:44:27Z", "author": "Arush James", "text": "I am your 1000th like", "likes": 0}, "replies": [{"id": "UgyUb2WoyZY39gFFG0x4AaABAg.8xjZ8ZFWmsb8xjZA1OZXg_", "publishedAt": "2019-07-23T16:44:38Z", "author": "Arush James", "text": ":)", "likes": 0}]}, {"topLevelComment": {"id": "UgwjZrmbKifrB-A3PxF4AaABAg", "publishedAt": "2019-07-17T18:56:59Z", "author": "Akshay Sapra", "text": "In the white house example, can you find the 'a' tag directly without first finding the 'h2' tag?", "likes": 1}, "replies": [{"id": "UgwjZrmbKifrB-A3PxF4AaABAg.8xVLY4WUM2k8xww7AK8snF", "publishedAt": "2019-07-28T21:24:05Z", "author": "hizoka andou", "text": "Yes, but it will list ALL a tags, and we just wanted the ones that come inside the h2 tags.", "likes": 0}]}, {"topLevelComment": {"id": "UgyqFJNGopK2HqTacAR4AaABAg", "publishedAt": "2019-07-12T23:44:05Z", "author": "Rhishabh Deshpande", "text": "A very neat and cogent description of the Beautiful Soup documentation. Thanks for this video, makes learning much more easier! :)", "likes": 8}}, {"topLevelComment": {"id": "UgzAJNMG6F6A1O9WaqF4AaABAg", "publishedAt": "2019-07-11T23:33:15Z", "author": "robin vermillion", "text": "answering my own question . pip install lxml", "likes": 123}, "replies": [{"id": "UgzAJNMG6F6A1O9WaqF4AaABAg.8xGOODUhlYl9M2PFsFZJtp", "publishedAt": "2021-04-12T18:47:43Z", "author": "Randy Miskuski", "text": "@Kristaps Did you ever solve this issue? I am running into the same one", "likes": 0}, {"id": "UgzAJNMG6F6A1O9WaqF4AaABAg.8xGOODUhlYl9HnDdtuiuuy", "publishedAt": "2020-12-28T01:52:51Z", "author": "Ingo Leeck", "text": "@Project Vibe same", "likes": 0}, {"id": "UgzAJNMG6F6A1O9WaqF4AaABAg.8xGOODUhlYl9GoXkr6WM47", "publishedAt": "2020-12-03T17:36:25Z", "author": "Kristaps", "text": "raise FeatureNotFound(\r\nbs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\nOk solved this error, but now i have another one:\n\nAttributeError: 'NoneType' object has no attribute 'attrs'", "likes": 1}, {"id": "UgzAJNMG6F6A1O9WaqF4AaABAg.8xGOODUhlYl9F16C2EKPxV", "publishedAt": "2020-10-20T07:02:10Z", "author": "cipmylo69", "text": "legend", "likes": 0}, {"id": "UgzAJNMG6F6A1O9WaqF4AaABAg.8xGOODUhlYl9Cwe7l-J1kD", "publishedAt": "2020-08-29T11:09:11Z", "author": "Project Vibe", "text": "i used html.parser\n-_-", "likes": 3}]}, {"topLevelComment": {"id": "UgyFIhJm-XD-BffwtXp4AaABAg", "publishedAt": "2019-07-11T23:27:16Z", "author": "robin vermillion", "text": "File \"untitled\", line 40, in <module>\r\n    soup = BeautifulSoup(src, 'lxml')\r\n  File \"C:\\Users\\Rob\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\bs4\\__init__.py\", line 196, in __init__\r\n    % \",\".join(features))\r\nbs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\ndo you know what might be causing this traceback error?", "likes": 0}, "replies": [{"id": "UgyFIhJm-XD-BffwtXp4AaABAg.8xGNhR_KgxD9GT6IY3rk3x", "publishedAt": "2020-11-25T00:33:11Z", "author": "Derek S", "text": "\u200b@LightSwitch Thank you!", "likes": 0}, {"id": "UgyFIhJm-XD-BffwtXp4AaABAg.8xGNhR_KgxD9367VXFugqg", "publishedAt": "2019-12-28T23:33:03Z", "author": "LightSwitch", "text": "i know im late, but try html.parser instead of 'lxml'", "likes": 0}]}, {"topLevelComment": {"id": "UgzR1lioOekxjbyzo0t4AaABAg", "publishedAt": "2019-07-05T21:12:19Z", "author": "Cosmetolog D", "text": "Thanks for video, really enjoyed!\nP.s. subscribed", "likes": 0}}, {"topLevelComment": {"id": "UgwVH0qNf-RzrZECoRR4AaABAg", "publishedAt": "2019-06-12T20:12:05Z", "author": "\u0430\u0440\u0442\u0443\u0440 \u043c\u0430\u0433\u043e\u043c\u0435\u0434\u043e\u0432", "text": "beautiful", "likes": 0}}, {"topLevelComment": {"id": "UgwlopL6cdmRw-KW6_F4AaABAg", "publishedAt": "2019-06-10T19:51:33Z", "author": "Shivam Gupta", "text": "when running this code :\nh2 = soup.find_all('h2')\n\nI m not able to scrap any of the lists for \"h2\" class while I' m able scrap for \"a\".\nkindly reply", "likes": 0}}, {"topLevelComment": {"id": "UgxajUoOkF7vI47NGkV4AaABAg", "publishedAt": "2019-06-07T10:02:00Z", "author": "Karen Wong", "text": "very easy to follow! clear explanation! thx for the tutorial!", "likes": 0}}, {"topLevelComment": {"id": "UgxnUt65DhOeZ9AvVad4AaABAg", "publishedAt": "2019-06-05T10:53:05Z", "author": "Jean DAVID", "text": "i wonder how I really did get the scrape.html into my pc to test the python scripts !!!!!", "likes": 0}}, {"topLevelComment": {"id": "Ugyl5CFhdTVlUbvSVWh4AaABAg", "publishedAt": "2019-05-25T00:21:17Z", "author": "Lohe221", "text": "16:00", "likes": 0}}, {"topLevelComment": {"id": "UgwagDQPNH9zs7QUn0x4AaABAg", "publishedAt": "2019-05-14T18:50:53Z", "author": "Nour SIDAOUI", "text": "This is the fourth web scraping tutorial and its by far the most beginner friendly one.\nPerfectly clear and well simplified!\nYou've done a great job man!\nPeace", "likes": 34}}, {"topLevelComment": {"id": "UgxrWEgotnKgeDymJyp4AaABAg", "publishedAt": "2019-05-08T23:24:40Z", "author": "Peter Steele", "text": "I really enjoyed the video, however I have some feedback regarding what was said. Around the 5-6 min mark you are talking about creating a variable \"soup\" to store the object information in. \"Soup\" is an object being instantiated from the BeautifulSoup class itself and is not a variable. Also you mention that  \"find_all\" is a method to the BeautifulSoup object above which is a class not an object. You do correctly reference \"soup\" as an object after the fact but I only wanted to point this out as it may be confusing for some viewers. I find its important to use the terminology correctly to help those learning to understand what is going on, and when we mix things up when making these videos, it can often lead to confusion. I wont pick apart the whole video, but just wanted to point out a couple of the early things I noticed. To me its like calling a method a function even though they are similar they are not the same. Please don't get upset over me pointing this out. I just want to help make the terminology consistent with what is going on so you can make more accurate videos for those learning off of these in the future. Thanks again for the tutorial!", "likes": 5}}, {"topLevelComment": {"id": "UgzRJdzQTfHiS_Apo7h4AaABAg", "publishedAt": "2019-04-25T02:53:59Z", "author": "Andrew Klauber", "text": "CAPTIAN HAMPTON YOU SEXY BEAST", "likes": 0}}, {"topLevelComment": {"id": "UgzOXdh0egat9eV0lIV4AaABAg", "publishedAt": "2019-04-08T15:32:01Z", "author": "Vrushabh", "text": "My name is Vrushabh Raut ! I am basically working as a freelancer ! I need a help to make a video on reviews of data scraping \nsoftware ! Basically i need full 5 minute review video that shows how to use the software ! If you are interested kindly reply on my\nemail given below ! It is beneficial for me to promote the site by the medium of you ! Kindly reply me sir !", "likes": 0}, "replies": [{"id": "UgzOXdh0egat9eV0lIV4AaABAg.8tTUaPY2ffM8tTUdmOP2YN", "publishedAt": "2019-04-08T15:32:29Z", "author": "Vrushabh", "text": "E-mail -ash.vrush.1996@gmail.com", "likes": 0}]}, {"topLevelComment": {"id": "Ugx5-vd4yIcB0muIdPB4AaABAg", "publishedAt": "2019-03-28T01:57:54Z", "author": "Andrea DWS", "text": "I need to work through a huge 10k html page and i need to select a lot of tables with different classe and text. i want to kill myself", "likes": 3}}, {"topLevelComment": {"id": "UgzRZwWh3EVXuRDQQah4AaABAg", "publishedAt": "2019-02-13T04:18:07Z", "author": "Jessie Gon\u017aalez", "text": "Hello I first started this tutorial last month or so and my terminal came up with \n\n\"soup = BeautifulSoup(src, \"lxml\")\nFile \"/Library/Python/2.7/site-packages/bs4/__init__.py\", line 152, in __init__\n% \",\".join(features))\nbs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\"\n\nRe-attempted, copied the code from your git repo, nada.\n\nI've been wanting to create some projects, and I especially like the idea of webscrapping.\n\nToday I re-attempted by starting over, and the error above came up. Crap, now what.\n\nI googled it and found this a Stack Overflow article with the same issue: https://stackoverflow.com/questions/24398302/bs4-featurenotfound-couldnt-find-a-tree-builder-with-the-features-you-requeste \n\nFix: sudo pip install lxml\n*close all terminals and re-navigate to the file.\n\nViola!\n\nThe power of Google, I'm so juiced! Thanks for the amazing video!", "likes": 12}, "replies": [{"id": "UgzRZwWh3EVXuRDQQah4AaABAg.8rHEYH3NVWw98Y5Ew07nOX", "publishedAt": "2020-05-12T02:49:01Z", "author": "Rishab Rao", "text": "ouch you spent a month on this sry dude", "likes": 0}, {"id": "UgzRZwWh3EVXuRDQQah4AaABAg.8rHEYH3NVWw8zDaFNKjL1K", "publishedAt": "2019-08-29T14:30:42Z", "author": "Eyal Segal", "text": "in the terminal: \npip install lxml\n\nhope that works :)", "likes": 2}, {"id": "UgzRZwWh3EVXuRDQQah4AaABAg.8rHEYH3NVWw8wYAjjmAAER", "publishedAt": "2019-06-24T00:48:55Z", "author": "Wright Ian", "text": "also remember to add \".text\" \nto a requests response\n\neg\nsoup = BeautifulSoup(src.text, \"lxml\")\nor\nsoup = BeautifulSoup(src, \"html5lib\")", "likes": 1}, {"id": "UgzRZwWh3EVXuRDQQah4AaABAg.8rHEYH3NVWw8wJ5SHsNyBY", "publishedAt": "2019-06-18T04:14:06Z", "author": "fadhil abdillah", "text": "@Wendy Lin thanks a lot sir :D", "likes": 0}, {"id": "UgzRZwWh3EVXuRDQQah4AaABAg.8rHEYH3NVWw8wHf8Ikdqru", "publishedAt": "2019-06-17T14:56:12Z", "author": "Wendy Lin", "text": "@fadhil abdillah you can probably try changing lxml into 'html.parser'\nnot sure if it'll work for you but it worked for me. hope it helps:)", "likes": 3}]}, {"topLevelComment": {"id": "UgyLigfMDXLQ9Qxco2t4AaABAg", "publishedAt": "2019-02-07T23:33:21Z", "author": "Boka Bosiljcic", "text": "If I comment \"This is beautiful \", would I be stating the obvious? :) Great work, keep it up. Will subscribe to your channel.", "likes": 2}, "replies": [{"id": "UgyLigfMDXLQ9Qxco2t4AaABAg.8r3qzCA7bwI97k6pYWwnxf", "publishedAt": "2020-04-22T07:41:37Z", "author": "odd number", "text": "the code doesnt work", "likes": 0}, {"id": "UgyLigfMDXLQ9Qxco2t4AaABAg.8r3qzCA7bwI8sVKwwTKdQY", "publishedAt": "2019-03-15T12:14:48Z", "author": "LucidProgramming", "text": "Thank you, Boka! I really appreciate the support and I hope you found my channel to be useful!", "likes": 0}]}, {"topLevelComment": {"id": "UgzgerbCSPY9Wn8diXR4AaABAg", "publishedAt": "2019-01-14T11:47:21Z", "author": "Dhruv Pathi", "text": "could you please make a tutorial for \nscikit learn", "likes": 1}}, {"topLevelComment": {"id": "UgyKGcYqP9gYBwzeP694AaABAg", "publishedAt": "2019-01-11T13:39:32Z", "author": "Dwane Ho", "text": "Hi can ask what kind of potato get system are you using for this video? It looks like Linux, but it looks not like Ubuntu, and also can you explain why you are using this? Thanks!", "likes": 0}}, {"topLevelComment": {"id": "UgwIcH4QNLftsoWBYUN4AaABAg", "publishedAt": "2019-01-08T00:50:02Z", "author": "luu vinh tuong", "text": "amazing tutorial which is very helpful", "likes": 3}, "replies": [{"id": "UgwIcH4QNLftsoWBYUN4AaABAg.8ppA6kBa7c98ptsi7nD4oV", "publishedAt": "2019-01-09T20:45:27Z", "author": "LucidProgramming", "text": "Thank you, Gabriel! Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python.\nhttp://bit.ly/lucidcode\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email", "likes": 0}]}, {"topLevelComment": {"id": "UgxnTzkr8Q0hNHPV87B4AaABAg", "publishedAt": "2019-01-07T14:15:30Z", "author": "Christine Ochoa", "text": "2:08   6:58 find links content <a class = to a>", "likes": 0}}, {"topLevelComment": {"id": "UgxVyx4gKmoUPffiOmx4AaABAg", "publishedAt": "2019-01-07T04:10:08Z", "author": "Prashant Arora", "text": "Hey please make a typescript tutorial video bcz there is no any good resource for it except official documentation", "likes": 4}, "replies": [{"id": "UgxVyx4gKmoUPffiOmx4AaABAg.8pmxDQ6fUnL8po1-QMMkjZ", "publishedAt": "2019-01-07T14:11:09Z", "author": "LucidProgramming", "text": "Hi Prashant. I will put that on my list of projects :) Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python.\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email\n\nCheers!", "likes": 0}]}, {"topLevelComment": {"id": "UgxBCpvHcaLONJUC32d4AaABAg", "publishedAt": "2019-01-05T23:02:26Z", "author": "Chariaf Dev", "text": "Great course thank you", "likes": 2}, "replies": [{"id": "UgxBCpvHcaLONJUC32d4AaABAg.8pjpCsLXW4C8pkDTPdXKA1", "publishedAt": "2019-01-06T02:43:08Z", "author": "LucidProgramming", "text": "Thank you, Ray! Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python.\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email\n\nCheers!", "likes": 1}]}, {"topLevelComment": {"id": "UgwYIq-X-jtare52JIh4AaABAg", "publishedAt": "2019-01-05T11:10:24Z", "author": "Rohit Verma", "text": "Can you plz upload a video on Kotlin and Android Development", "likes": 1}, "replies": [{"id": "UgwYIq-X-jtare52JIh4AaABAg.8piYiq7MeQf8plOUUxLFoJ", "publishedAt": "2019-01-06T13:38:38Z", "author": "LucidProgramming", "text": "@Rohit Verma Awesome, thank you, Rohit! :)\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email", "likes": 0}, {"id": "UgwYIq-X-jtare52JIh4AaABAg.8piYiq7MeQf8pkSJqtukXH", "publishedAt": "2019-01-06T04:52:54Z", "author": "LucidProgramming", "text": "@Rohit Verma Thank you, Rohit! I appreciate your support!", "likes": 0}, {"id": "UgwYIq-X-jtare52JIh4AaABAg.8piYiq7MeQf8pkQB0vlnjf", "publishedAt": "2019-01-06T04:34:13Z", "author": "Rohit Verma", "text": "Thanks for your suggestion.I'll definetly subscribe to your channel.", "likes": 1}, {"id": "UgwYIq-X-jtare52JIh4AaABAg.8piYiq7MeQf8pkJ1vnfVId", "publishedAt": "2019-01-06T03:31:49Z", "author": "LucidProgramming", "text": "Android development and Kotlin is on my list! Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python.\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email\n\nCheers!", "likes": 1}]}, {"topLevelComment": {"id": "UgylM9vxyuXfhW8CHz54AaABAg", "publishedAt": "2019-01-05T09:07:51Z", "author": "Amilcar C. da Silva", "text": "Amazing tutorial video. Fantastic! Well explained....thanks a lot tutor....I really like this tutorial...I have learnt about this web scraping in my class of data mining....I was not really clear about this point.....but this video tutorial with the coding in python makes me better understand .....once again huge thanks to you.....You render great service to many.", "likes": 4}, "replies": [{"id": "UgylM9vxyuXfhW8CHz54AaABAg.8piKhJE2--u8pkDb6opzbs", "publishedAt": "2019-01-06T02:44:19Z", "author": "LucidProgramming", "text": "Thank you so much Amilcar, I really appreciate those kind words!  Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python.\nhttp://bit.ly/lucidcode\n\nActually, if you're interested in staying up-to-date with the content on my channel, you might want to sign up for the mailing list I'm starting:\nhttp://bit.ly/lp_email\n\nCheers!", "likes": 0}]}, {"topLevelComment": {"id": "UgwvqIpQoNN2WJjWb9d4AaABAg", "publishedAt": "2019-01-05T00:12:09Z", "author": "Kerry Weston", "text": "Instead of using the White House website, can you use Amazon?  I'd like to see how to collect all the product data from Amazon and store it in a database.   This would be useful.", "likes": 29}, "replies": [{"id": "UgwvqIpQoNN2WJjWb9d4AaABAg.8phNOd-OCeY8u91Q5aeGWp", "publishedAt": "2019-04-25T13:23:39Z", "author": "LucidProgramming", "text": "@Phlpp That is on my channel now actually! It's here: \nhttps://www.youtube.com/watch?v=x2r_RmvfzRo&list=PL5tcWHG-UPH1fnJw-BvBiiiPUPm1LUKsm\n\nCheers!", "likes": 1}, {"id": "UgwvqIpQoNN2WJjWb9d4AaABAg.8phNOd-OCeY8u8hSHqvenH", "publishedAt": "2019-04-25T10:20:27Z", "author": "Phlpp", "text": "@LucidProgramming I am still waiting and hoping ;) Any nes about that project?", "likes": 0}, {"id": "UgwvqIpQoNN2WJjWb9d4AaABAg.8phNOd-OCeY8q-Zd7Nmg7O", "publishedAt": "2019-01-12T11:04:41Z", "author": "Giuseppe L'Episcopo", "text": "@Alok Mishra Couldn't you just import the CSV in mySQL and make a db out of it?", "likes": 1}, {"id": "UgwvqIpQoNN2WJjWb9d4AaABAg.8phNOd-OCeY8pjEY_Jm7Zu", "publishedAt": "2019-01-05T17:33:20Z", "author": "Alok Mishra", "text": "I don't know how to store the data into databases but I knew how to make the CSV file of it", "likes": 0}, {"id": "UgwvqIpQoNN2WJjWb9d4AaABAg.8phNOd-OCeY8phRF8HNo99", "publishedAt": "2019-01-05T00:45:48Z", "author": "LucidProgramming", "text": "I actually have series on my channel that I'm planning to record soon on this. Stay tuned! :)", "likes": 4}]}, {"topLevelComment": {"id": "UgxkuRljijL0jMUsnIh4AaABAg", "publishedAt": "2019-01-04T22:55:44Z", "author": "ria roussou", "text": "I like programming \ud83d\ude00", "likes": 5}, "replies": [{"id": "UgxkuRljijL0jMUsnIh4AaABAg.8phEdwWXWzk8pysEqKt-Ia", "publishedAt": "2019-01-11T19:17:31Z", "author": "ria roussou", "text": "thanks for letting me know", "likes": 0}, {"id": "UgxkuRljijL0jMUsnIh4AaABAg.8phEdwWXWzk8phRCt5I_GY", "publishedAt": "2019-01-05T00:45:29Z", "author": "LucidProgramming", "text": "You and me both :) Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 1}]}, {"topLevelComment": {"id": "UgwxwlUgScCA07TR5v54AaABAg", "publishedAt": "2019-01-04T20:58:49Z", "author": "raffaele rimorso", "text": "good tutorials", "likes": 1}, "replies": [{"id": "UgwxwlUgScCA07TR5v54AaABAg.8ph1GeOsl8z8phRGSV5yHu", "publishedAt": "2019-01-05T00:45:59Z", "author": "LucidProgramming", "text": "Thank you, Raffaele! Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgxOLpAnzxoPUVMUsMt4AaABAg", "publishedAt": "2019-01-04T18:43:28Z", "author": "Kashish Charaya", "text": "on Instagram  @t0nystark007.......If you like meme's please have a look at our page and follow us for more... We're posting daily and share our page with your friends... It means a lot to us .. thank you......", "likes": 0}}, {"topLevelComment": {"id": "Ugww1RQPaxmHkz-5wl14AaABAg", "publishedAt": "2019-01-04T17:49:49Z", "author": "BeAPickle", "text": "This is how you get banned by Google =D", "likes": 1}}, {"topLevelComment": {"id": "Ugzs5B9WrKgEVYTZQcB4AaABAg", "publishedAt": "2019-01-04T17:02:20Z", "author": "Host Promo", "text": "Vim is the master editor!", "likes": 4}, "replies": [{"id": "Ugzs5B9WrKgEVYTZQcB4AaABAg.8pgbCb4APLs8rYiXIf0tAC", "publishedAt": "2019-02-19T23:15:57Z", "author": "LucidProgramming", "text": "@korgman2k7 No problem, cheers!", "likes": 0}, {"id": "Ugzs5B9WrKgEVYTZQcB4AaABAg.8pgbCb4APLs8rYAlzp8koJ", "publishedAt": "2019-02-19T18:12:15Z", "author": "korgman2k7", "text": "\u200b@LucidProgramming Thanks for linking your vimrc in the other video", "likes": 1}, {"id": "Ugzs5B9WrKgEVYTZQcB4AaABAg.8pgbCb4APLs8pgdVam9rm-", "publishedAt": "2019-01-04T17:22:24Z", "author": "LucidProgramming", "text": "Vim for the Win! :) Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 1}]}, {"topLevelComment": {"id": "Ugxy90WIINp9R_l-e9R4AaABAg", "publishedAt": "2019-01-04T16:20:13Z", "author": "MK MK", "text": "*Where's the soup at though?*", "likes": 4}, "replies": [{"id": "Ugxy90WIINp9R_l-e9R4AaABAg.8pgXO4TMYQP8pgdSZW-LPi", "publishedAt": "2019-01-04T17:21:59Z", "author": "LucidProgramming", "text": "Hmm, perhaps that's a bit of false advertising on my part :P \n\nThanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgxlN2HodbrcqD8UI3h4AaABAg", "publishedAt": "2019-01-04T16:19:58Z", "author": "Sihab Portal", "text": "which software   you use for write code?", "likes": 1}, "replies": [{"id": "UgxlN2HodbrcqD8UI3h4AaABAg.8pgXMGN8v408pgb1TvipgG", "publishedAt": "2019-01-04T17:00:49Z", "author": "Random Dev", "text": "That's vim", "likes": 0}, {"id": "UgxlN2HodbrcqD8UI3h4AaABAg.8pgXMGN8v408pgYgUOqGN9", "publishedAt": "2019-01-04T16:31:36Z", "author": "LucidProgramming", "text": "Hi Sihab! I use Vim. If you like to see how I have it set up for Python development, I have a whole playlist on that you might find helpful:\nhttp://bit.ly/lp_vim\n\nThanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. I've also got some Vim videos you may enjoy.  Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 1}]}, {"topLevelComment": {"id": "Ugw3ndx6jvtGqPwcxeh4AaABAg", "publishedAt": "2019-01-04T15:59:28Z", "author": "Gigel Chiazna", "text": "wht not regex al urls in page source?", "likes": 0}, "replies": [{"id": "Ugw3ndx6jvtGqPwcxeh4AaABAg.8pgV07tGfqi8pgVEKWCe3I", "publishedAt": "2019-01-04T16:01:24Z", "author": "LucidProgramming", "text": "Hi Gigel. I'm not sure I understand your question. Why am I not using Regex in the page source? Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 1}]}, {"topLevelComment": {"id": "Ugy6XSr824XkxuRxDed4AaABAg", "publishedAt": "2019-01-04T15:56:04Z", "author": "Priyadarshini Chettiar", "text": "Thank you so much.. was eagerly waiting for this tutorial.. :)", "likes": 1}, "replies": [{"id": "Ugy6XSr824XkxuRxDed4AaABAg.8pgUcEEx6Gm8pgV7CQBJD8", "publishedAt": "2019-01-04T16:00:26Z", "author": "LucidProgramming", "text": "No problem, Priyadarshini! Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgxZqenjNKMGY0r_P3h4AaABAg", "publishedAt": "2019-01-04T15:54:27Z", "author": "runningonthestreets", "text": "Go Vim", "likes": 6}, "replies": [{"id": "UgxZqenjNKMGY0r_P3h4AaABAg.8pgURNE8W9T8pgVBH7BWug", "publishedAt": "2019-01-04T16:00:59Z", "author": "LucidProgramming", "text": "Vim for the Win! :) Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel! I have lots of other similar web scraping and automation videos in Python. I've also got some Vim videos you may enjoy.  Cheers! :)\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgwPaO9xdpw8Fu7OMC14AaABAg", "publishedAt": "2019-01-04T15:41:27Z", "author": "Kali Battula", "text": "Fourth comment\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\nThank u very much for the lecture\ud83d\udc4d\ud83d\udc4d\ud83d\udc4c\ud83d\udc4c", "likes": 1}, "replies": [{"id": "UgwPaO9xdpw8Fu7OMC14AaABAg.8pgSxCh9aFx8pgULqaC0ge", "publishedAt": "2019-01-04T15:53:41Z", "author": "LucidProgramming", "text": "@Kali Battula Thank you, Kali! I appreciate the support :)", "likes": 1}, {"id": "UgwPaO9xdpw8Fu7OMC14AaABAg.8pgSxCh9aFx8pgU2Qj45hX", "publishedAt": "2019-01-04T15:51:02Z", "author": "Kali Battula", "text": "@LucidProgramming Subscribed just know \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\u263a\ufe0f\u263a\ufe0f", "likes": 1}, {"id": "UgwPaO9xdpw8Fu7OMC14AaABAg.8pgSxCh9aFx8pgTIlF2RJO", "publishedAt": "2019-01-04T15:44:32Z", "author": "LucidProgramming", "text": "No problem, Kali. I hope you enjoy the content! If you find it useful, you might find some other similar content on my channel:\nhttp://bit.ly/lucidcode\n\nCheers, and thanks for watching!", "likes": 1}]}, {"topLevelComment": {"id": "UgxguQJ-rRULeShB1El4AaABAg", "publishedAt": "2019-01-04T15:39:41Z", "author": "MR.REBELLIO", "text": "Second", "likes": 3}, "replies": [{"id": "UgxguQJ-rRULeShB1El4AaABAg.8pgSkITGrX58pgTov6C7jr", "publishedAt": "2019-01-04T15:49:03Z", "author": "LucidProgramming", "text": "So close to first :P Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel!\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgxawTo2k-bUWCaSEhx4AaABAg", "publishedAt": "2019-01-04T15:39:20Z", "author": "Vinay chinchkar", "text": "Hiie", "likes": 2}, "replies": [{"id": "UgxawTo2k-bUWCaSEhx4AaABAg.8pgShjCcZK08pgTqrS_PwD", "publishedAt": "2019-01-04T15:49:19Z", "author": "LucidProgramming", "text": "Hiii!! :) Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel!\nhttp://bit.ly/lucidcode", "likes": 0}]}, {"topLevelComment": {"id": "UgwqvrgkKZC6OFEJg0B4AaABAg", "publishedAt": "2019-01-04T15:39:07Z", "author": "EasternTeasers", "text": "First", "likes": 1}, "replies": [{"id": "UgwqvrgkKZC6OFEJg0B4AaABAg.8pgSg4GlZQS8pgTsxAJJRD", "publishedAt": "2019-01-04T15:49:36Z", "author": "LucidProgramming", "text": "Someone else may have had you beat, but only slightly :P Thanks for watching and for the comment. If you like this type of content, please do consider checking out my YouTube channel!\nhttp://bit.ly/lucidcode", "likes": 0}]}]}