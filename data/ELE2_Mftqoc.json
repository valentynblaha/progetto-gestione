{"video": {"id": "ELE2_Mftqoc", "title": "Reinforcement Learning Course - Full Machine Learning Tutorial", "description": "Reinforcement learning is an area of machine learning that involves taking right action to maximize reward in a particular situation. In this full tutorial course, you will get a solid foundation in reinforcement learning core topics.\n\nThe course covers Q learning, SARSA, double Q learning, deep Q learning, and policy gradient methods. These algorithms are employed in a number of environments from the open AI gym, including space invaders, breakout, and others. The deep learning portion uses Tensorflow and PyTorch.\n\nThe course begins with more modern algorithms, such as deep q learning and policy gradient methods, and demonstrates the power of reinforcement learning.\n\nThen the course teaches some of the fundamental concepts that power all reinforcement learning algorithms. These are illustrated by coding up some algorithms that predate deep learning, but are still foundational to the cutting edge. These are studied in some of the more traditional environments from the OpenAI gym, like the cart pole problem.\n\n\ud83d\udcbbCode: https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning\n\n\u2b50\ufe0f Course Contents \u2b50\ufe0f\n\u2328\ufe0f (00:00:00) Intro \n\u2328\ufe0f (00:01:30) Intro to Deep Q Learning \n\u2328\ufe0f (00:08:56) How to Code Deep Q Learning in Tensorflow \n\u2328\ufe0f (00:52:03) Deep Q Learning with Pytorch Part 1: The Q Network \n\u2328\ufe0f (01:06:21) Deep Q Learning with Pytorch part 2: Coding the Agent \n\u2328\ufe0f (01:28:54) Deep Q Learning with Pytorch part\n\u2328\ufe0f (01:46:39) Intro to Policy Gradients  3: Coding the main loop \n\u2328\ufe0f (01:55:01) How to Beat Lunar Lander with Policy Gradients \n\u2328\ufe0f (02:21:32) How to Beat Space Invaders with Policy Gradients \n\u2328\ufe0f (02:34:41) How to Create Your Own Reinforcement Learning Environment Part 1 \n\u2328\ufe0f (02:55:39) How to Create Your Own Reinforcement Learning Environment Part 2 \n\u2328\ufe0f (03:08:20) Fundamentals of Reinforcement Learning \n\u2328\ufe0f (03:17:09) Markov Decision Processes \n\u2328\ufe0f (03:23:02) The Explore Exploit Dilemma \n\u2328\ufe0f (03:29:19) Reinforcement Learning in the Open AI Gym: SARSA \n\u2328\ufe0f (03:39:56) Reinforcement Learning in the Open AI Gym: Double Q Learning \n\u2328\ufe0f (03:54:07) Conclusion \n\nCourse from Machine Learning with Phil. Check out his YouTube channel: https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw\n\n--\n\nLearn to code for free and get a developer job: https://www.freecodecamp.org\n\nRead hundreds of articles on programming: https://medium.freecodecamp.org", "duration": "PT3H55M27S", "likes": "6731", "views": "369862"}, "comments": [{"topLevelComment": {"author": "Machine Learning with Phil", "text": "Here are some time stamps folks!\n\nIntro 00:00:00\nIntro to Deep Q Learning 00:01:30\nHow to Code Deep Q Learning in Tensorflow 00:08:56\nDeep Q Learning with Pytorch Part 1: The Q Network 00:52:03\nDeep Q Learning with Pytorch part 2: Coding the Agent 01:06:21\nDeep Q Learning with Pytorch part 3: Coding the main loop 01:28:54\nIntro to Policy Gradients 01:46:39\nHow to Beat Lunar Lander with Policy Gradients 01:55:01\nHow to Beat Space Invaders with Policy Gradients 02:21:32\nHow to Create Your Own Reinforcement Learning Environment Part 1 02:34:41\nHow to Create Your Own Reinforcement Learning Environment Part 2 02:55:39\nFundamentals of Reinforcement Learning 03:08:20\nMarkov Decision Processes 03:17:09\nThe Explore Exploit Dilemma 03:23:02\nReinforcement Learning in the Open AI Gym: SARSA 03:29:19\nReinforcement Learning in the Open AI Gym: Double Q Learning 03:39:56\nConclusion 03:54:07", "likes": 145}, "replies": [{"author": "Lucas Vanover", "text": "\ud83d\ude22\ud83d\ude22", "likes": 0}, {"author": "\u0418\u0432\u0430\u043d \u0420\u0443\u0434\u0430\u043a\u043e\u0432", "text": "\u0417\u044e", "likes": 0}, {"author": "Ahmed Ray official", "text": "Ppppp", "likes": 0}, {"author": "Sushi Boy", "text": "Ooooo", "likes": 0}, {"author": "FireLord Zaki", "text": "I litetally love this guy i subscribed right after i watched this. Best guy to watch while in college.", "likes": 2}]}, {"topLevelComment": {"author": "Wyphone Ma", "text": "this is a terrible tutorial, he is just typing. I think it would be better if he typed all the code already and save the time to explain the codes deeply", "likes": 0}}, {"topLevelComment": {"author": "NIKLAS DAMM", "text": "25.10.22  22:30", "likes": 0}}, {"topLevelComment": {"author": "Shah Fahad", "text": "Is  there any course (Reinforcement Learning code based) for beginners ?", "likes": 0}}, {"topLevelComment": {"author": "0GRANATE0", "text": "How comes that I am not capable to UNDERSTAND this stuff? Is it OK to accept that I am dumb? Serious question... (I am a software engineer, did computer science, but had always problem with math, but did always the best mark when I had to deliver a project..)", "likes": 0}}, {"topLevelComment": {"author": "0GRANATE0", "text": "When I wanted to implement a multi agent reinforcement learning envirenment for a soccer game (multiple agents, separately trained, with separate models) what algorithms would I use for a continious envirenment (not a grid world) where the players can walk/run/shot everywhere? - DQN Reinforcement Learning?", "likes": 0}}, {"topLevelComment": {"author": "Alexey Smirnov", "text": "1.4 hours of coding without any execution. this is not a good way of learning", "likes": 0}}, {"topLevelComment": {"author": "that_endo", "text": "2nd time waking up to this guy", "likes": 0}}, {"topLevelComment": {"author": "honkmangoose", "text": "Steve from blue's clues wasn't joking when he said he was going to college", "likes": 0}}, {"topLevelComment": {"author": "Ankit K Pandey", "text": "Not useful. Creator should at least run a code once before wrapping up a section. Small syntax/logical errors are fine but when there is error in code itself (eg: new_state_batch in first section) and there isn't much explanation about it, whole hour spent in coding goes for nothing.", "likes": 0}}, {"topLevelComment": {"author": "Emanuele Papucci", "text": "my utils package do not have any plotLearning func...", "likes": 0}}, {"topLevelComment": {"author": "Zo\u00eb Rycroft", "text": "for anyone watching, inheriting from object is implied, you haven't needed to type that since even the oldest versions of python 3, save yourself some time ;) `class foo(object):` is exactly the same as `class foo:`\n\nthe reason he types it here is probably for intercompatability between py2 and py3, but not even a year after this was uploaded py2 went end of life, so you shouldn't need to worry about that anymore :))", "likes": 0}}, {"topLevelComment": {"author": "Lahiru Prasangika", "text": "H\u1d0f\u1d1b\u1d07\u029f M\u1d00\u0274\u1d00\u0262\u1d07\u1d0d\u1d07\u0274\u1d1b S\u028fs\u1d1b\u1d07\u1d0d\ud83d\udcf2\u2764\ufe0f\ud83d\udcbb\n\n\ud83d\udc9a \u1d05\u1d0f\u0274'\u1d1b \u1d0d\u026ass \u1d21\u1d00\u1d1b\u1d04\u029c \u1d1b\u029c\u1d07 \u1d20\u026a\u1d05\u1d07\u1d0f \u2714\n         \"s\u1d1c\u0299s\u1d04\u0280\u026a\u0299\u1d07 & s\u029c\u1d00\u0280\u1d07\" \ud83d\udc4d\n\n \u029c\u1d0f\u1d1b\u1d07\u029f \u1d0d\u1d00\u0274\u1d00\u0262\u1d07\u1d0d\u1d07\u0274\u1d1b s\u028fs\u1d1b\u1d07\u1d0d\ud83d\udcf2\u2764\ufe0f\ud83d\udcbb| \u1d04\u1d0f\u1d05\u1d07s\u029c\u1d0f\u1d21 \u029f\u1d00\u1d18\u1d22 | 2022 \nhttps://youtu.be/9Uw8acxIWsY", "likes": 0}}, {"topLevelComment": {"author": "Charles Peck", "text": "easy to understand, much pleasure", "likes": 0}}, {"topLevelComment": {"author": "Sachin Yadav", "text": "dislike", "likes": 0}}, {"topLevelComment": {"author": "Amit Buch", "text": "Extremely well explained. Kudos to the tutor. Simple explanation to workign code in less than an hour is amazing and yet very clearly laid out. Thanks for this upload.", "likes": 5}}, {"topLevelComment": {"author": "Caribou Data Science", "text": "Reinforced learning,  is like when you have to write 1000 times \"I will not talk in class\"?", "likes": 1}}, {"topLevelComment": {"author": "Boxsewatio Doxsewstio", "text": "nice", "likes": 0}}, {"topLevelComment": {"author": "Claude De-Tchambila", "text": "This is one of the best free RL videos available. Please make some more.", "likes": 2}, "replies": [{"author": "Yushuo Wang", "text": "Indeed", "likes": 0}]}, {"topLevelComment": {"author": "Peter McFarland", "text": "This video has helped me find clues that ultimately helped me to understand machine learning. Thanks blue Steve.", "likes": 1}}, {"topLevelComment": {"author": "J\u00e1n \u0160prl\u00e1k", "text": "Beta tester od roku 2016 oce\u0148ujem,,\ud83d\udc96\ud83d\udc96\ud83d\udc9d", "likes": 0}}, {"topLevelComment": {"author": "Amber Rothermel", "text": "<Great content as usual.   I look forward to every single video you make. Thank you for helping all of us stay on top of the market. A lot has changed and that's on everything but the truth is I don't even care much about bullish or bearish market anymore because NAUGHTON   got me cover as I am comfortably  making  9.1 B T C monthly...", "likes": 18}, "replies": [{"author": "6_Darni Nugraheni", "text": "<Thank you for turning your unique perspective into a vision that we can all support and work toward. I\u2019m so proud to be part of this journey", "likes": 0}, {"author": "Shela Myranda.", "text": "I came across one of his client testimony, and I decided to add him up, I\u2019m glad he accepted, and guided me all through my trading, I recovered my lost , and made of over $44,000 profits", "likes": 0}, {"author": "Kaltum Habib", "text": "I was big on gold and silver but a few months ago I discovered Bitcoin and Ethereum. Trading with Naughton has been really helpful in my journey. This is an opportunity for newbies to capitalise  on,", "likes": 0}, {"author": "Ben stafford", "text": "What impresses me most about Naughton  is how well he explains the basics concept of winning before actually letting you use his trade signals. This goes a long way to ensure winning trades.", "likes": 0}, {"author": "Hayden Craig", "text": "My first investment with Naughton  gave me the assurance that have made me invest without the fear of losing, I got four of my friends involved with him already...", "likes": 0}]}, {"topLevelComment": {"author": "ANYIN PAUL", "text": "How do I get this Deep Q-learning codes on github", "likes": 0}}, {"topLevelComment": {"author": "\u179f\u17c1\u1784 \u17a0\u17ca\u17b6\u1784", "text": "\u17a2\u179a\u1782\u17bb\u178e", "likes": 0}}, {"topLevelComment": {"author": "  Marcos Diego ", "text": "<Great info  4btc got me back  13.8btc. This is insane , I highly recommend Naughton  , you are far & away the most genuine, educational, and sincere individual I've come across . I know I'm not alone in being grateful for your  insight/knowledge day after day.< this is one of the best medium to backup your assets incase it goes bearish\u2026 Cheers & thank you for what you do!", "likes": 0}}, {"topLevelComment": {"author": "Joe C", "text": "12 minutes in and scratching my head. Hi yes I'm lost", "likes": 0}}, {"topLevelComment": {"author": "Matthew Schneider", "text": "That\u2019s \u201cClassic Phil!\u201d.", "likes": 0}}, {"topLevelComment": {"author": "Tim Raiser", "text": "I get errors when i run the pip install commands for box2d-py, tensorflow-gpu and torch. Is there something i'm missing?", "likes": 0}}, {"topLevelComment": {"author": "T J", "text": "Classic example of a great coder who can't teach :)", "likes": 0}}, {"topLevelComment": {"author": "Pakheria", "text": "raise error.UnregisteredEnv(\"No registered env with id: {}\".format(id))\r\ngym.error.UnregisteredEnv: No registered env with id: Breakout-v0", "likes": 0}}, {"topLevelComment": {"author": "The Game Changer", "text": "Just need to get this over.\nAfter I find her,\nThat's it.\nI will share the technique.\nShe must code it.\nThen its done.\nI will go to desert.\n\nIf your thinking nuclear reactor?\nIn the middle of the congested city in the world with rascal scientist like me .\n\nIt will result to catastrophe.", "likes": 0}}, {"topLevelComment": {"author": "Mohammadreza Shariat", "text": "most boring course I have ever watched", "likes": 0}, "replies": [{"author": "Henry Powell", "text": "lol yah for starting 5 mins i thought its gonna be good but then lol", "likes": 0}]}, {"topLevelComment": {"author": "ENTWAZE", "text": "LMAO, im just watching him code at this point pretending i understand", "likes": 0}, "replies": [{"author": "Henry Powell", "text": "lol yah", "likes": 0}]}, {"topLevelComment": {"author": "fabrizio antonazzo", "text": "grazie mille ho iniziato da poco a seguire il tuo corso ben fatto", "likes": 0}}, {"topLevelComment": {"author": "JTKMBA", "text": "When you mentioned a course on Full Machine Learning Tutorial - Reinforcement Learning and there is no proper order to it. I don't recommend watching this thing. There are tons of materials that are a lot simpler than this.", "likes": 0}}, {"topLevelComment": {"author": "Finarwa \u134a\u1293\u122d\u12cb ", "text": "I really appreciate the work you are doing . Could you mention which  the development tool you are \nusing for the whole series?", "likes": 1}, "replies": [{"author": "Benyam", "text": "Vscode", "likes": 1}]}, {"topLevelComment": {"author": "Natasha Samuel", "text": "Great class.\nKeep up the good work.\n\nThank You,\nNatasha Samuel", "likes": 0}, "replies": [{"author": "Henry Powell", "text": "so you understood everything in the tutorial?", "likes": 0}]}, {"topLevelComment": {"author": "\uba4b\uc9c4\ub0a8\uc790", "text": "I saw this video \nI'll see this video again \nThank you", "likes": 0}}, {"topLevelComment": {"author": "David Shook", "text": "The Explore Exploit Dilemma,  volume drops significantly", "likes": 0}}, {"topLevelComment": {"author": "David Shook", "text": "are you preaching, are you lazy, your face and wavy hands aren't helping me learn. Code is often blurry at 360P, are you catering to 4k viewers with un-shared 100 megabit connections, I've seen better presentations at 360P.", "likes": 0}, "replies": [{"author": "Henry Powell", "text": "\"with un-shared 100 megabit connections\" lol that's me but I didn't get the tutorial either", "likes": 0}]}, {"topLevelComment": {"author": "Wayne Kerr", "text": "Can you please provide an environment.yml file for this?", "likes": 0}}, {"topLevelComment": {"author": "Aarya Sankhe", "text": "Anyone interested in learning the terminologies of what he is talking about should go check out the video lectures Stanford did on MDPs(Markov decisions processes and RL), they're about each an hour long and do go in depth behind the math for a lot of this stuff. Cheers!!!", "likes": 15}, "replies": [{"author": "The Almighty Anthony Analog", "text": "Underrated. Thank you.", "likes": 0}]}, {"topLevelComment": {"author": "Evan Stark", "text": "man i just fell asleep on youtube and now i\u2019ve been watching this for 2 hours 19 minutes", "likes": 3}, "replies": [{"author": "danilkos", "text": "I sleep to this all night. YouTube knows I\u2019m using it as white noise at bedtime and offers sleepy stuff like this  \ud83d\ude2c", "likes": 0}]}, {"topLevelComment": {"author": "tariq shah", "text": "Is there anyone who knows which version of TensorFlow is used in this video?", "likes": 0}, "replies": [{"author": "Lawrence", "text": "Looks like TF1", "likes": 1}]}, {"topLevelComment": {"author": "Haneul Kim", "text": "Amazing course, thanks alot Phil! One question, you were comparing policy gradient methods with reinforcement learning however after few searches it seems like policy gradient method is an algorithm within RL. Could you clarify?", "likes": 0}, "replies": [{"author": "poro ro", "text": "\u04360", "likes": 0}]}, {"topLevelComment": {"author": "Renato N", "text": "Yep... He is theaching for who already knows.....", "likes": 7}}, {"topLevelComment": {"author": "Dev Sinharoy", "text": "when you do def(build network) wouldnt it be easier just to use keras", "likes": 0}}, {"topLevelComment": {"author": "\u0130brahim Salih", "text": "Information is so good but the background is grabbing the attention away from the information.", "likes": 0}}, {"topLevelComment": {"author": "Deep Morzaria", "text": "this is not for beginners !!!", "likes": 1}}, {"topLevelComment": {"author": "Luci0", "text": "Phil, you are a fucking boss :>", "likes": 0}}, {"topLevelComment": {"author": "Dummy PG", "text": "This is the only issue that i often see on any \"basic tutorial\" videos. There's no explaination on the terminologies during the intros.", "likes": 38}, "replies": [{"author": "constantinetikh", "text": "The terminology and concepts are explained in the two blocks starting 03:08:20", "likes": 3}]}, {"topLevelComment": {"author": "Shahina Shaikh", "text": "okay this is not for beginners at all. wow.", "likes": 1}}, {"topLevelComment": {"author": "Remon Kamal", "text": "Good awsome view and awsome channel, good work", "likes": 0}}, {"topLevelComment": {"author": "Mindflayer Gaming", "text": "ok guys where is the code available", "likes": 0}}, {"topLevelComment": {"author": "42", "text": "Thank you profoundly for sharing your knowledge!", "likes": 1}}, {"topLevelComment": {"author": "motbus", "text": "Is this tf1 or tf2?", "likes": 1}, "replies": [{"author": "Kae", "text": "tf1", "likes": 0}]}, {"topLevelComment": {"author": "AngryCoder", "text": "Clearly the video is only for people who have already researched about RL. Not for beginners at all!", "likes": 2}}, {"topLevelComment": {"author": "InturnetHaetMachine", "text": "This is a great video if you already understand the topic, understand the code and just want a guy saying what he's typing out aloud, kinda explaining bits and pieces here and there.", "likes": 51}, "replies": [{"author": "Ramtin Nazeryan", "text": "He said at the beginning no need to know about this and that. 14 minutes into the video he is typing the line 123. Honestly why didn't he copy and paste it? :))))", "likes": 2}, {"author": "Henry Powell", "text": "yah and people in the comment section be like: thank you, what a great tutorial for free, great explanation, while they get nothing and just being smart in typing a comment", "likes": 2}]}, {"topLevelComment": {"author": "Bill CX", "text": "Do you know why his kernel size is even number? Normally we use odd number for easy calculation.", "likes": 2}, "replies": [{"author": "Bill CX", "text": "\u200b@Mantas Because a non-symmetric kernel (even number) yields a non-symmetric filter response. In the example above, this non-symmetry leads to a shift of the blurred image by half a pixel.", "likes": 0}, {"author": "Mantas", "text": "why is that a difference?", "likes": 0}]}, {"topLevelComment": {"author": "C", "text": "what can you recommend to watch of your other youtube videos before watching this one?", "likes": 9}, "replies": [{"author": "Kae", "text": "Check out his channel\n\nhis original one.", "likes": 0}, {"author": "Matthias Muller", "text": "python guides and some linear algebra to understand what is happening", "likes": 1}]}, {"topLevelComment": {"author": "C", "text": "are you using tensor flow 2.0?", "likes": 1}, "replies": [{"author": "Kae", "text": "No, it's tensorflow 1.4.10. If you want to look at tensorflow 2.0 or keras, go check out Phil's YouTube channel", "likes": 0}]}, {"topLevelComment": {"author": "sagar lama", "text": "I'm a beginner and the Background loop seems more interesting than what he's talking. I hope I understand what he's saying someday", "likes": 14}, "replies": [{"author": "Musique Lover", "text": "The same thing", "likes": 1}]}, {"topLevelComment": {"author": "Barbell Bender", "text": "The length of the flatten outputlayer can actually be calculated from first conv layer tracing the data through the network. Just use the function:\n \n((dimension length - kernal size for the dimension + 2*padding)/stride)+1 = output length for the dimension\n\n\ndo this for each dimension for each conv layer and multiply by number of outputs in the end to find the length of the flat dimension as such:\n\n\n1st conv layer: ((185 - 8 + 2*1)/4) + 1 =  44 (acutally 44.75 but you always round down, since there are no 0.75 pixels)\n                           ((95 - 8 + 2*1)/ 4) + 1 = 22 (rounded down from 22.25)\n\n\n2nd conv: ((44 - 4 + 2*0)/2) + 1 = 21\n                  ((22 - 4 + 2*0)/2) + 1 = 10\n\n\n3rd conv: ((21 - 3 + 2*0)/1) + 1 = 19\n                 ((10 - 3 + 2*0)/1) + 1 = 8\n\n\nthis means the 3rd layer outputs 128 frames with each having dimensions 19*8 and therefore if you wanted to flatten them into one you would get one dimension with 128*19*8 vectors.\nJust neat little trick for those who want it", "likes": 4}}, {"topLevelComment": {"author": "Cactus_Duke", "text": "What program do you use to run your code?", "likes": 0}, "replies": [{"author": "Kae", "text": "It's the linux distribution native terminal, not sure about the exact distribution. Maybe ubuntu", "likes": 0}]}, {"topLevelComment": {"author": "SB", "text": "Why are we onehot encoding the actions?", "likes": 0}}, {"topLevelComment": {"author": "Robi Hamdani", "text": "i feel uncomfortable with his hands", "likes": 3}}, {"topLevelComment": {"author": "Hnin Nwe", "text": "Please use some illustrations so that can understand more easily", "likes": 0}}, {"topLevelComment": {"author": "azlaan samad", "text": "The stacking frame reshaping code is wrong, it messes up the entire array. I am trying to debug it.", "likes": 0}}, {"topLevelComment": {"author": "azlaan samad", "text": "Hello Phil, I think there is another mistake in the code, in the learn function it should be reward_batch + gamma*np.max(Q_next, axis=1)*(1-terminal_batch) instead of just terminal_batch. Since we are passing int(done) as a stored observation. Therefore for done=False, int(done)=0 and vice versa. And if episode does not end that is done equals False then we need to add the next Q_value otherwise we only add reward. What do you think? Am I correct?", "likes": 1}, "replies": [{"author": "KRISHNENDU BOSE", "text": "Could you post the link to the lectures here?", "likes": 0}, {"author": "Kae", "text": "yeah i do think so. but i encourage you to try it both ways. sometime it trains the agent to finish the episode as fast as possible, that's not what we want.", "likes": 0}]}, {"topLevelComment": {"author": "Carlos Alberto Leyva", "text": "How exactly do I run these python programs?\nI'm using Atom IDE", "likes": 5}, "replies": [{"author": "Kae", "text": "You can run via the terminal(linux and macOS)/command prompt(windows) or you can download the Run Script addon in Atom(that's what i use)", "likes": 0}]}, {"topLevelComment": {"author": "Blaine Haws", "text": "I am currently creating an agent-based model that will generate x numver of agents. Each agent has a step function. I would LOVE to incorporate this reinforced learning method into the model. How would you adjust it from taking a visual frame like from a game to using only the global environment variables? Is it simply as easy as swapping out one for the other?", "likes": 1}}, {"topLevelComment": {"author": "azlaan samad", "text": "self.q = tf.reduce_sum(tf.multiply(self.Q_values, self.actions)) \nWhy are you doing this? I fail to understand the meaning of this line?? Thank you in advance :)", "likes": 0}, "replies": [{"author": "azlaan samad", "text": "@\u94ed  I see so many other tutorials which do the same thing, and I fail to understand why.", "likes": 0}, {"author": "\u94ed", "text": "he made a mistake.... the github code is corrected.", "likes": 1}]}, {"topLevelComment": {"author": "vsp", "text": "\u2b50\ufe0f Course Contents \u2b50\ufe0f \u2328\ufe0f (00:00:00) Intro \u2328\ufe0f (00:01:30) Intro to Deep Q Learning \u2328\ufe0f (00:08:56) How to Code Deep Q Learning in Tensorflow \u2328\ufe0f (00:52:03) Deep Q Learning with Pytorch Part 1: The Q Network \u2328\ufe0f (01:06:21) Deep Q Learning with Pytorch part 2: Coding the Agent \u2328\ufe0f (01:28:54) Deep Q Learning with Pytorch part \u2328\ufe0f (01:46:39) Intro to Policy Gradients 3: Coding the main loop \u2328\ufe0f (01:55:01) How to Beat Lunar Lander with Policy Gradients \u2328\ufe0f (02:21:32) How to Beat Space Invaders with Policy Gradients \u2328\ufe0f (02:34:41) How to Create Your Own Reinforcement Learning Environment Part 1 \u2328\ufe0f (02:55:39) How to Create Your Own Reinforcement Learning Environment Part 2 \u2328\ufe0f (03:08:20) Fundamentals of Reinforcement Learning \u2328\ufe0f (03:17:09) Markov Decision Processes \u2328\ufe0f (03:23:02) The Explore Exploit Dilemma \u2328\ufe0f (03:29:19) Reinforcement Learning in the Open AI Gym: SARSA \u2328\ufe0f (03:39:56) Reinforcement Learning in the Open AI Gym: Double Q \n\nLearning \u2328\ufe0f (03:54:07) Conclusion", "likes": 4}}, {"topLevelComment": {"author": "SAEED ALAFIFI", "text": "Thanks for your information \nCan you help me to set an environment  to trade forex and how to set it with metatrader 4\nPlease help me", "likes": 0}}, {"topLevelComment": {"author": "Gilfoyle", "text": "Heads up: this one isn't for beginners.", "likes": 104}, "replies": [{"author": "opkp", "text": "@Black Hole lel , got him good", "likes": 1}]}, {"topLevelComment": {"author": "SmartassEyebrows", "text": "One minor correction for those watching at 1:19:12 and trying to follow along (like myself): on line 77 after the \"else\", the \"memStart = int(np.random.choice(range(self.memCntr - batch_size - 1)))\" should actually be \"memStart = int(np.random.choice(range(self.memSize - batch_size - 1)))\". \n\n\n\nThe self.memSize is needed here instead of self.memCntr because at this point the self.memory list is now full (the \"else\" branch), but the self.memCntr value is continuing to grow and is now larger than the max self.memory size. That leads to line 78 giving miniBatch an empty list, [ ], leading to memory being an empty array, because memStart will be a larger value than the self.memory list length, while then being used as the index for grabbing the miniBatch from that same self.memory list -- no good. Ultimately that leads to an exception: \"too many indices for array\" on line 81 (since we are trying to forward an empty 1D numpy array and call 2D indices that don't exist). With self.memSize for line 77, that no longer happens, and memStart stays within the bounds of the self.memory length/size. With that, everything works, and you can watch the agent play :)", "likes": 11}}, {"topLevelComment": {"author": "Ayush Shukla", "text": "I have that T-Shirt", "likes": 1}}, {"topLevelComment": {"author": "FireLord Zaki", "text": "Wow fcc really stepping up", "likes": 2}}, {"topLevelComment": {"author": "Abhishek Jain", "text": "i can hardly hear anything", "likes": 0}}, {"topLevelComment": {"author": "Lal Omega", "text": "This is awesome..", "likes": 0}}, {"topLevelComment": {"author": "Mmanuel", "text": "YEET!", "likes": 1}}, {"topLevelComment": {"author": "j o", "text": "so it seems I can't even install this or get started, I'm on windows 10 and I have python 3.7 working, I installed pip, gym, but when I got to tensorflow it's telling me I have \"no matching distribution found for tensorflow-gpu\", some people suggested that it's because I got the latest version of py, others suggest to use anaconda, what should I do?", "likes": 0}, "replies": [{"author": "RRKS101_1 TF2", "text": "@j o I had some issues with tensorflow-gpu but I think that is because it didn't support cuda 10.1, installing tensorflow-nightly (I think it's called that) fixed it", "likes": 0}, {"author": "Koga Master", "text": "Use Google Colaboratory", "likes": 0}, {"author": "Koga Master", "text": "Use Google colaboratory", "likes": 0}, {"author": "Machine Learning with Phil", "text": "@j o Sorry, just seeing this now. To the best of my knowledge, Tensorflow only works with python 3.6. You can just install 3.6 in parallel with other versions, without nuking the whole install, I believe.", "likes": 0}, {"author": "j o", "text": "lol after buckling and trying anaconda tensorflow fails, but everything else runs properly. Really fun tutorial.", "likes": 0}]}, {"topLevelComment": {"author": "Joe Barco", "text": "Is this something someone can learn without previous experience?", "likes": 0}, "replies": [{"author": "Kae", "text": "Not really, i suggest you read about reinforcement learning basics before you watch this.", "likes": 1}, {"author": "Machine Learning with Phil", "text": "You sure can, I'll cover the basics as we go along.", "likes": 1}]}, {"topLevelComment": {"author": "Kerry Weston", "text": "Thank you. Great job in explaining the content.", "likes": 2}}, {"topLevelComment": {"author": "Muhammad Ahmed", "text": "please make complete course on WordPress 5.2", "likes": 0}}, {"topLevelComment": {"author": "Sihab Portal", "text": "machine learning is really trending matter", "likes": 0}}, {"topLevelComment": {"author": "Project YYSY", "text": "Can I really get a job after learning stuff like machine learning and etc online? I did BSc in EEE but as I was not interested I did no do well and did not learn much, only thing I somewhat enjoyed was the C and C++ courses and digital logic design and verilog/vhdl courses . Later I did python course from MITx and enjoyed solving all the exercises. Pls give me hope.", "likes": 0}, "replies": [{"author": "Project YYSY", "text": "wild doggo appears. wild doggo regrets wasting his time.", "likes": 0}]}, {"topLevelComment": {"author": "Abhishek shah", "text": "How much linear algebra and statistics should I know for this track?", "likes": 0}, "replies": [{"author": "Kae", "text": "not much alegbra and statistics i can say, but more about the basic reinforcement learning terms", "likes": 0}, {"author": "Machine Learning with Phil", "text": "Not much is needed. I explain it all as we go along.", "likes": 3}]}, {"topLevelComment": {"author": "ANOUBHAV AGARWAAL be16b002", "text": "Omg thank you soo much", "likes": 1}}, {"topLevelComment": {"author": "Imtiaz Ahmad", "text": "thanks, so helpful  video", "likes": 1}}, {"topLevelComment": {"author": "Just Tech", "text": "This is AWESOME! \ud83d\udc4d\ud83d\udc4d\ud83d\udc4d Thank you for this!", "likes": 4}}, {"topLevelComment": {"author": "Vasanth RS", "text": "Can you please make a full tutorial in flutter?\nThanks, I've been watching your tutorial for a long time.\nExcellent work.", "likes": 0}, "replies": [{"author": "Vasanth RS", "text": "@freeCodeCamp.org Thank you so much,\nI'm looking forward for that tutorial.", "likes": 0}, {"author": "Bala Chandran", "text": "Thanks", "likes": 0}, {"author": "freeCodeCamp.org", "text": "We're posting a full flutter tutorial next week. :)", "likes": 2}]}, {"topLevelComment": {"author": "Nick Lansbury", "text": "You had me at \"Atari\"!!!", "likes": 7}}, {"topLevelComment": {"author": "Subhadeep Banerjee", "text": "Add something with udemy", "likes": 2}}, {"topLevelComment": {"author": "Umair Nehri", "text": "\ud83d\udcaf\ud83d\udcaf\ud83d\udcaf", "likes": 1}}, {"topLevelComment": {"author": "Kush Choudhary", "text": "Let me tell you\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is so good :)", "likes": 6}}, {"topLevelComment": {"author": "tejas Jani", "text": "I'm second", "likes": 1}, "replies": [{"author": "Shirish", "text": "@tejas Jani we will give you even you don't want...", "likes": 1}, {"author": "tejas Jani", "text": "@Shirish no thanks.", "likes": 1}, {"author": "Bala Chandran", "text": "Lol @shirish", "likes": 1}, {"author": "Shirish", "text": "Congratulations you both got prize money of 5k dollars", "likes": 3}, {"author": "tejas Jani", "text": "No \nI'm First", "likes": 1}]}, {"topLevelComment": {"author": "Ashok Rathore", "text": "1st viewer\ud83d\ude0e\ud83d\ude0e", "likes": 1}, "replies": [{"author": "Kush Choudhary", "text": "@Ashok Rathore \ud83d\ude05 It's great bro", "likes": 1}, {"author": "Ashok Rathore", "text": "@Kush Choudhary i was second dude sorry\ud83d\udc4d", "likes": 2}, {"author": "Kush Choudhary", "text": "Sorry to tell but it was me", "likes": 3}]}, {"topLevelComment": {"author": "Anmol Gupta", "text": "First", "likes": 0}}]}